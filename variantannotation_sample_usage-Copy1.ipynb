{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Variant Annotation, Filtering, Prioritization\n",
    "## Sample usage of variantannotation and VarP packages as a pipeline for new epitope prediction\n",
    "\n",
    "#### Author: C. Mazzaferro\n",
    "#### Email: cmazzafe@ucsd.edu\n",
    "#### Date: June 2016\n",
    " \n",
    "## Outline of Notebook\n",
    "<a id = \"toc\"></a>\n",
    "1. <a href = \"#background\">Background</a>\n",
    "2. <a href = \"#setup\">Set Up File and Libraries</a>\n",
    "3. <a href = \"#ANNOVAR\">Run Annovar</a>\n",
    "4. <a href = \"#myvariant\">Obtain data from myvariant.info</a>\n",
    "5. <a href = \"#filter\">Variant Filtering & File Creation</a>\n",
    "    * <a href = \"#tumorvars\">Rare Tumor Variant Filter</a>\n",
    "    * <a href = \"#diseasevars\">Rare Diesease Variant Filter</a>\n",
    "    * <a href = \"#caddvars\">CADD PHRED High Impact Variants</a>\n",
    "    * <a href = \"#own\">Create Your Own Filter</a>\n",
    "6. <a href = \"#export\">Export CSV and VCF files</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"background\"></a>\n",
    "## Background\n",
    "\n",
    "This notebook will walk you through the steps of how variants coming from a VCF can be annotated efficiently and thoroughly using the package variantannotation. In particular, the package is aimed at providing a way of retrieving variant information using [ANNOVAR](http://annovar.openbioinformatics.org/en/latest/) and [myvariant.info](myvariant.info) and consolidating it in conveninent formats. It is well-suited for bioinformaticians interested in aggregating variant information into a single database for ease of use and to provide higher analysis capabities. \n",
    "\n",
    "The aggregation is performed specifically by structuring the data in lists of python dictionaries, with each variant being described by a multi-level dictionary. The choice was made due to the inconsistencies that exist between data availability, and the necessity to store their information in a flexible manner. Further, this specific format permits its parsing to a MongoDb instance (dictionaries are the python representation of JSON objects), which enables the user to efficiently store, query and filter such data. \n",
    "\n",
    "Finally, the package also has the added functionality to create csv and vcf files from MongoDB. The class Filters allows the user to rapidly query data that meets certain criteria as a list of documents, and the class FileWriter can transform such list into more widely accepted formats such as vcf and csv files. It should be noted that here, the main differential the package offers is the ability to write these files preserving all the annotation data. In the vcf files, for instance, outputs will have a 'Otherinfo' column where all the data coming from ANNOVAR and myvariant.info is condensed (while still preserving its structure). For vcf files, outputs will have around ~120-200 columns, depending on the amount of variant data that can be retrieved from myvvariant.info. \n",
    "\n",
    "\n",
    "**Notes on required software**\n",
    "\n",
    "the following libraries will be install upon installing variantannotation:\n",
    "- myvariant\n",
    "- pysam\n",
    "- pymongo\n",
    "- pyvcf\n",
    "\n",
    "Other libraries that are needed, but should natively e installed on most OS: \n",
    "\n",
    "- Watchdog\n",
    "- Pandas\n",
    "- Numpy\n",
    "\n",
    "Further, a MongoDB database must be set up. Refer to the documentation page for more information. \n",
    "Similarly, ANNOVAR must be downloaded, alongside with its supporting databases (also listed on the documentation page)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"setup\"></a>\n",
    "## Import libraries and specify file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlomazzaferro/anaconda/lib/python2.7/site-packages/matplotlib/__init__.py:1035: UserWarning: Duplicate key in file \"/Users/carlomazzaferro/.matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import vcf\n",
    "import time\n",
    "import pysam\n",
    "import myvariant\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "#variantannotation functions\n",
    "from variantannotation import annotate_batch\n",
    "from variantannotation import create_output_files\n",
    "from variantannotation import myvariant_parsing_utils\n",
    "from variantannotation import mongo_DB_export\n",
    "from variantannotation import utilities\n",
    "from variantannotation import MongoDB_querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ANNOVAR_PATH = '/data/annovar/'\n",
    "FILE_NAMES = ['Tumor_RNAseq_variants.vcf', 'Tumor_targeted_seq.vcf', 'normal_targeted_seq.vcf', 'normal_blood_WGS.vqsr.vcf', 'somatic_mutect_old.vcf']\n",
    "IN_PATH = '/data/ccbb_internal/interns/Carlo/test_vcf/'\n",
    "OUT_PATH = '/data/ccbb_internal/interns/Carlo/test_vcf_out/'\n",
    "vcf_file = IN_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/ccbb_internal/interns/Carlo/test_vcf/Tumor_RNAseq_variants.vcf\n",
      "/data/ccbb_internal/interns/Carlo/test_vcf/Tumor_targeted_seq.vcf\n",
      "/data/ccbb_internal/interns/Carlo/test_vcf/normal_targeted_seq.vcf\n",
      "/data/ccbb_internal/interns/Carlo/test_vcf/normal_blood_WGS.vqsr.vcf\n",
      "/data/ccbb_internal/interns/Carlo/test_vcf/somatic_mutect_old.vcf\n"
     ]
    }
   ],
   "source": [
    "#Check if file paths are correctly pointing to the specified files.\n",
    "for i in range(0, len(FILE_NAMES)):\n",
    "    print IN_PATH+FILE_NAMES[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"ANNOVAR\"></a>\n",
    "## Run Annovar \n",
    "\n",
    "This will run ANNOVAR. A csv file named tumortargcsvout.hg19_multianno.csv will appear in the OUT_PATH specified. The csv file can then be processed and integrated with the data coming from myvariant.info. \n",
    "This command may take a some time to run (5-30 minutes for each file depending on file size).\n",
    "To keep things simple, we can start by looking at one file only. Let's run annovar on it. In any case, if you have multiple files to work on, you can run them in parallel by running the block after the next one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently working on VCF file: Tumor_RNAseq_variants, field avinput\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field variant_function\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field exonic_variant_function\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_tfbsConsSites\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_cytoBand\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_targetScanS\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_genomicSuperDups\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_gwasCatalog\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_esp6500siv2_all_filtered\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_esp6500siv2_all_dropped\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field 2015_08_filtered\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field 2015_08_dropped\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_snp138_filtered\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_snp138_dropped\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_ljb26_all_filtered\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_ljb26_all_dropped\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_cg46_filtered\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_cg46_dropped\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_cg69_filtered\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_cg69_dropped\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_popfreq_all_filtered\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_popfreq_all_dropped\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_clinvar_20140929_filtered\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_clinvar_20140929_dropped\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_cosmic70_filtered\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_cosmic70_dropped\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_nci60_filtered\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_nci60_dropped\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field csv\n",
      "\n",
      "Annovar finished working on file : Tumor_RNAseq_variants has finished. A .csv file has been created in the \n",
      " OUT_PATH directory\n",
      "Currently working on VCF file: Tumor_targeted_seq, field avinput\n",
      "Currently working on VCF file: Tumor_targeted_seq, field variant_function\n",
      "Currently working on VCF file: Tumor_targeted_seq, field exonic_variant_function\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_tfbsConsSites\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_cytoBand\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_targetScanS\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_genomicSuperDups\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_gwasCatalog\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_esp6500siv2_all_filtered\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_esp6500siv2_all_dropped\n",
      "Currently working on VCF file: Tumor_targeted_seq, field 2015_08_filtered\n",
      "Currently working on VCF file: Tumor_targeted_seq, field 2015_08_dropped\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_snp138_filtered\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_snp138_dropped\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_ljb26_all_filtered\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_ljb26_all_dropped\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_cg46_filtered\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_cg46_dropped\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_cg69_filtered\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_cg69_dropped\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_popfreq_all_filtered\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_popfreq_all_dropped\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_clinvar_20140929_filtered\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_clinvar_20140929_dropped\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_cosmic70_filtered\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_cosmic70_dropped\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_nci60_filtered\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_nci60_dropped\n",
      "Currently working on VCF file: Tumor_targeted_seq, field csv\n",
      "\n",
      "Annovar finished working on file : Tumor_targeted_seq has finished. A .csv file has been created in the \n",
      " OUT_PATH directory\n",
      "Currently working on VCF file: normal_targeted_seq, field avinput\n",
      "Currently working on VCF file: normal_targeted_seq, field variant_function\n",
      "Currently working on VCF file: normal_targeted_seq, field exonic_variant_function\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_tfbsConsSites\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_cytoBand\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_targetScanS\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_genomicSuperDups\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_gwasCatalog\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_esp6500siv2_all_filtered\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_esp6500siv2_all_dropped\n",
      "Currently working on VCF file: normal_targeted_seq, field 2015_08_filtered\n",
      "Currently working on VCF file: normal_targeted_seq, field 2015_08_dropped\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_snp138_filtered\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_snp138_dropped\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_ljb26_all_filtered\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_ljb26_all_dropped\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_cg46_filtered\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_cg46_dropped\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_cg69_filtered\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_cg69_dropped\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_popfreq_all_filtered\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_popfreq_all_dropped\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_clinvar_20140929_filtered\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_clinvar_20140929_dropped\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_cosmic70_filtered\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_cosmic70_dropped\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_nci60_filtered\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_nci60_dropped\n",
      "Currently working on VCF file: normal_targeted_seq, field csv\n",
      "\n",
      "Annovar finished working on file : normal_targeted_seq has finished. A .csv file has been created in the \n",
      " OUT_PATH directory\n",
      "Currently working on VCF file: normal_blood_WGS, field avinput\n",
      "Currently working on VCF file: normal_blood_WGS, field variant_function\n",
      "Currently working on VCF file: normal_blood_WGS, field exonic_variant_function\n",
      "Currently working on VCF file: normal_blood_WGS, field hg19_tfbsConsSites\n",
      "Currently working on VCF file: normal_blood_WGS, field hg19_cytoBand\n",
      "Currently working on VCF file: normal_blood_WGS, field hg19_targetScanS\n",
      "Currently working on VCF file: normal_blood_WGS, field hg19_genomicSuperDups\n",
      "Currently working on VCF file: normal_blood_WGS, field hg19_gwasCatalog\n",
      "Currently working on VCF file: normal_blood_WGS, field hg19_esp6500siv2_all_filtered\n",
      "Currently working on VCF file: normal_blood_WGS, field hg19_esp6500siv2_all_dropped\n",
      "Currently working on VCF file: normal_blood_WGS, field 2015_08_filtered\n",
      "Currently working on VCF file: normal_blood_WGS, field 2015_08_dropped\n",
      "Currently working on VCF file: normal_blood_WGS, field hg19_snp138_filtered\n",
      "Currently working on VCF file: normal_blood_WGS, field hg19_snp138_dropped\n"
     ]
    }
   ],
   "source": [
    "utilities.run_annovar(ANNOVAR_PATH, IN_PATH+FILE_NAMES[0], OUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Annovar runs as a subprocess on every file. They will run in parallel for speed up. \n",
    "for i in range(0, len(FILE_NAMES)):\n",
    "    utilities.run_annovar(ANNOVAR_PATH, IN_PATH+FILE_NAMES[i], OUT_PATH)\n",
    "\n",
    "#This serves to give a real-time feedback of the ANNOVAR progress and status. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the name and location of the csv file that ANNOVAR produces as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "filepath_out = '/data/ccbb_internal/interns/Carlo/test_vcf_out/'\n",
    "filepath_in = '/data/ccbb_internal/interns/Carlo/test_vcf/'\n",
    "\n",
    "#For safety, check the files in directory. Either run '!ls' here on iPython, or go to the directory and check \n",
    "#manually for existing files. There should be once csv file for every vcf file. \n",
    "\n",
    "VCF_FILE_NAME = 'Tumor_RNAseq_variants.vcf'\n",
    "CSV_FILE_NAME = 'Tumor_RNAseq_variants.hg19_multianno.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"myvariant\"></a>\n",
    "## Getting data from myvariant.info\n",
    "The package offers 4 different methods to obtain variant data. Two of them require annovar, while the other two are based solely on the use of myvariant.info service. The latter can be used without having to worry about downloading and installing annovar databases, but it tends to return partial or no information for some variants. \n",
    "\n",
    "The different methods also enable the user to decide how the data will be parsed to MongoDB. 1 and 3 parse the data by chunks: the user specifies a number of variants (usually 1000), and the data from the vcf and csv files are parsed as soon as those 1000 variants are processed and integrated. This enables huge files to be processed without having to hold them in memory and potentially cause a Memory Overflow error. \n",
    "\n",
    "Methods 2 and 4, on the other hand, process the files on their entirety and send them to MongoDB at once. Well-suited for smaller files. See docs for more info. \n",
    "\n",
    "## Export data to MongoDB by chunks, iteratively. \n",
    "\n",
    "For this tutorial, we will use method #1. Data from annovar (as a csv file) will be obtained 1000 lines at a time, instead of attempting to parse and process an entire csv file at once.\n",
    "\n",
    "As soon as you run the scripts from variantannotaiton, variant data will be retrieved from myvariant.info and the data will automatically be integrated and stored to MongoDB. Database and collection name should be specified, and there must be a running MongoDB connection. The script will set up a client to communicate between python (through pymongo) and the the database.\n",
    "\n",
    "In general, the shell command:\n",
    "\n",
    "`mongod --dbpath ../data/db`  \n",
    "\n",
    "where data/db is the designated location where the data will be stored, will initiate MongoDB. After this, the script should store data to the directory automatically.\n",
    "For pymongo, and more information on how to set up a Mongo Database: https://docs.mongodb.com/getting-started/python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 1 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 2 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 3 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 4 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 5 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 6 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 7 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 8 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 9 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 10 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 11 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 12 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 13 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 14 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 15 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 16 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 17 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 18 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 19 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 20 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 21 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 22 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 23 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 24 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 25 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 26 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 27 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 28 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 29 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 30 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 31 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 32 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 33 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 34 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 35 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 36 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 37 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 38 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 39 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 40 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 41 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 42 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-2710...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 43 of 43\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished!'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunksize = 10000\n",
    "step = 0\n",
    "\n",
    "#Get variant list. Should always be the first step after running ANNOVAR\n",
    "open_file = myvariant_parsing_utils.VariantParsing()\n",
    "\n",
    "#Name Collections & DB. Change them to something appropriate. Each file should live in a collection\n",
    "db_name = 'Variant_Prioritization_Workflow'\n",
    "\n",
    "collection_name = 'Test_Tumor_RNAseq'\n",
    "\n",
    "list_file = open_file.get_variants_from_vcf(filepath_in+VCF_FILE_NAME)\n",
    "as_batch = annotate_batch.AnnotationMethods()\n",
    "as_batch.by_chunks(list_file, chunksize, step, filepath_out+CSV_FILE_NAME, collection_name, db_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"filter\"></a>\n",
    "## Variant Filtering & Output Files\n",
    "\n",
    "Here we implement three different filters that allow for the retrieval of specific variants. The filters are implemented as MongoDB queries, and are designed to provie the user with a set of relevant variants. In case the user would like to define its own querying, a template is provided. \n",
    "The output of the queries is a list of dictionaries (JSON documents), where each dictionary contains data reltive to one variant. \n",
    "\n",
    "Further, the package allows the user to parse these variants into an annotated csv or vcf file. \n",
    "If needed, annotated, unfiltered vcf and csv files can also be created. They will have the same length (number of variants) as the original files, but will contain much more complete annotation data coming from myvariant.info and ANNOVAR databases. \n",
    "\n",
    "To create a csv file, just the filtered output is needed. To create an annotated vcf file, a tab indexed file (.tbi) file is needed (see comments in  section Create unfiltered annotated vcf and csv files at the end of this page). This can be created using tabix.  \n",
    "\n",
    "First, the file needs to be compressed:\n",
    "\n",
    "From the command line, running:\n",
    "\n",
    "`bgzip -c input_file.vcf > input_file.vcf.gz`\n",
    "\n",
    "returns `input_vcf_file.vcf.gz`\n",
    "\n",
    "and running \n",
    "\n",
    "`tabix input_vcf_file.vcf.gz`\n",
    "\n",
    "will return: `input_vcf_file.vcf.gz.tbi`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"tumorvars\"></a>\n",
    "## Specifying cancer-specifc rare variants\n",
    "\n",
    " - filter 1: ThousandGenomeAll < 0.05 or info not available\n",
    " - filter 2: ESP6500siv2_all < 0.05 or info not available\n",
    " - filter 3: cosmic70 information is present\n",
    " - filter 4: Func_knownGene is exonic, splicing, or both\n",
    " - filter 5: ExonicFunc_knownGene is not \"synonymous SNV\"\n",
    " - filter 6: Read Depth (DP) > 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filepath = '/data/ccbb_internal/interns/Carlo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variants found that match rarity criteria: 11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished writing annotated, filtered VCF file'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create output files (if needed): specify name of files and path \n",
    "rare_cancer_variants_csv = filepath + \"/tumor_rna_rare_cancer_vars_csv.csv\"\n",
    "rare_cancer_variants_vcf = filepath + \"/tumor_rna_rare_cancer_vars_vcf.vcf\"\n",
    "input_vcf_compressed = filepath + '/test_vcf/Tumor_RNAseq_variants.vcf.gz'\n",
    "\n",
    "#Apply filter.\n",
    "filter_collection = MongoDB_querying.Filters(db_name, collection_name)\n",
    "rare_cancer_variants = filter_collection.rare_cancer_variant()\n",
    "\n",
    "#Crete writer object for filtered lists:\n",
    "my_writer = create_output_files.FileWriter(db_name, collection_name)\n",
    "\n",
    "#cancer variants filtered files\n",
    "my_writer.generate_annotated_csv(rare_cancer_variants, rare_cancer_variants_csv)\n",
    "my_writer.generate_annotated_vcf(rare_cancer_variants,input_vcf_compressed, rare_cancer_variants_vcf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read filtered vcf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from VarP import utils\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/carlomazzaferro/Documents/CCBB/neoantigen/VarP-master/')\n",
    "reader_names = ['Tumor_RNA_Reader', 'Tumor_Targeted_Reader',\n",
    "           'Normal_DNA_Reader', 'Normal_Blood_Reader',\n",
    "           'Somatic_Mutect_Reader']   #bug fixed in source\n",
    "\n",
    "path_to_files ='/Volumes/Seagate Backup Plus Drive/vcf_files/varcode_to_test/' \n",
    "myhandler = utils.HandleReaders(reader_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "### Create a list of variant collections. See varcode's documentation for more info on this type of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 2] No such file or directory: '/Volumes/Seagate Backup Plus Drive/vcf_files/varcode_to_test/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-875c2111fbbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist_collections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_collection_from_readers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_collections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Let's pick one of the collections to start working with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmy_collection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist_collections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/carlomazzaferro/Documents/CCBB/neoantigen/VarP-master/VarP/utils.pyc\u001b[0m in \u001b[0;36mcreate_collection_from_readers\u001b[0;34m(self, path_to_dir)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mreader\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \"\"\"\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mreader_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*.vcf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 2] No such file or directory: '/Volumes/Seagate Backup Plus Drive/vcf_files/varcode_to_test/'"
     ]
    }
   ],
   "source": [
    "list_collections = myhandler.create_collection_from_readers(path_to_files)\n",
    "\n",
    "print type(list_collections[4])\n",
    "#Let's pick one of the collections to start working with\n",
    "my_collection = list_collections[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create desired outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_coding_effects = myhandler.return_list_coding_effects(my_collection)   #list of coding effects\n",
    "protein_list = myhandler.return_protein_list(list_coding_effects)           #list of proteins\n",
    "dataframe = myhandler.return_dataframe(protein_list, list_coding_effects)   #dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prot</th>\n",
       "      <th>variants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MVSKLSQLQTELLAALLESGLSKEALIQALGEPGPYLLAGEGPLDK...</td>\n",
       "      <td>FrameShift(variant=chr12 g.121434630_121434631...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MVKSYLQQHNIPQREVVDTTGLNQSHLSQHLNKGTPMKTQKRAALY...</td>\n",
       "      <td>FrameShift(variant=chr12 g.121434630_121434631...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MEPSRALLGCLASAAAAAPPGEDGAGAGAEEEEEEEEAAAAVGPGE...</td>\n",
       "      <td>Deletion(variant=chr14 g.71275774_71275776delC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MEPSRALLGCLASAAAAAPPGEDGAGAGAEEEEEEEEAAAAVGPGE...</td>\n",
       "      <td>Deletion(variant=chr14 g.71275774_71275776delC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MEPSRALLGCLASAAAAAPPGEDGAGAGAEEEEEEEEAAAAVGPGE...</td>\n",
       "      <td>Deletion(variant=chr14 g.71275774_71275776delC...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                prot  \\\n",
       "0  MVSKLSQLQTELLAALLESGLSKEALIQALGEPGPYLLAGEGPLDK...   \n",
       "1  MVKSYLQQHNIPQREVVDTTGLNQSHLSQHLNKGTPMKTQKRAALY...   \n",
       "2  MEPSRALLGCLASAAAAAPPGEDGAGAGAEEEEEEEEAAAAVGPGE...   \n",
       "3  MEPSRALLGCLASAAAAAPPGEDGAGAGAEEEEEEEEAAAAVGPGE...   \n",
       "4  MEPSRALLGCLASAAAAAPPGEDGAGAGAEEEEEEEEAAAAVGPGE...   \n",
       "\n",
       "                                            variants  \n",
       "0  FrameShift(variant=chr12 g.121434630_121434631...  \n",
       "1  FrameShift(variant=chr12 g.121434630_121434631...  \n",
       "2  Deletion(variant=chr14 g.71275774_71275776delC...  \n",
       "3  Deletion(variant=chr14 g.71275774_71275776delC...  \n",
       "4  Deletion(variant=chr14 g.71275774_71275776delC...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Fasta file\n",
    "myhandler.generate_fasta_file(dataframe, path_to_files+'PEPTIDES.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Epitope Prediction\n",
    "#### Use epitope predict and netMHCpan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions done for 18 proteins in 1 alleles\n",
      "results saved to /Volumes/Seagate Backup Plus Drive/vcf_files/varcode_to_test/netmhciipan\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import epitopepredict as ep\n",
    "from epitopepredict import base, sequtils, analysis\n",
    "\n",
    "sns.set_context(\"notebook\", font_scale=1.4)\n",
    "fastafile =  path_to_files+'PEPTIDES.txt'\n",
    "\n",
    "#get data in DF format\n",
    "zaire = sequtils.fasta_to_dataframe(fastafile)\n",
    "\n",
    "P = base.get_predictor('netmhciipan')\n",
    "#P1 = base.getPredictor('netmhciipan')\n",
    "#P2 = base.getPredictor('netmhciipan')\n",
    "\n",
    "savepath1 = 'netmhciipan'\n",
    "#run prediction for several alleles and save results to savepath\n",
    "alleles = [\"HLA-DRB1*0101\"]#, \n",
    "           \n",
    "           #\"HLA-DRB1*0108\", \"HLA-DRB1*0305\", \"HLA-DRB1*0401\",\n",
    "           #\"HLA-DRB1*0404\", \"HLA-DRB3*0101\", \"HLA-DRB4*0104\"]\n",
    "\n",
    "P.predictProteins(zaire, length=11, alleles=alleles, path=savepath1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary: 10029 peptides in 8 proteins and 1 alleles\n",
      "                            peptide\n",
      "name                               \n",
      "Deletion(variant=chr14          686\n",
      "Deletion(variant=chr16         3689\n",
      "Deletion(variant=chr19         1447\n",
      "Deletion(variant=chr9          1558\n",
      "FrameShift(variant=chr12        364\n",
      "Insertion(variant=chr6          803\n",
      "Substitution(variant=chr19     1081\n",
      "Substitution(variant=chr2       401\n"
     ]
    }
   ],
   "source": [
    "P.load(path = savepath1)\n",
    "P.summarize()\n",
    "P.allele_summary()\n",
    "P.protein_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Deletion(variant=chr14'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.name.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no such protein name in binder data\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-acbcedafd4b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#get promiscuous binders in at least 2 alleles above 5 percentile cutoff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ZEBOVgp3'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpromiscuousBinders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcutoff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'promiscuous binders:'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/carlomazzaferro/anaconda/lib/python2.7/site-packages/epitopepredict/base.py\u001b[0m in \u001b[0;36mpromiscuousBinders\u001b[0;34m(self, binders, name, value, cutoff, n, unique_core)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mbinders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetBinders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcutoff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m'core'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbinders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m             \u001b[0mbinders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'core'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeptide\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0mgrps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'peptide'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'pos'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "#get promiscuous binders in at least 2 alleles above 5 percentile cutoff\n",
    "name='ZEBOVgp3'\n",
    "pb = P.promiscuousBinders(name=name,n=2,cutoff=5)\n",
    "print 'promiscuous binders:'\n",
    "print pb\n",
    "print '--------------------------'\n",
    "#binders sorted by median rank over all alleles, cutoff here is the median rank\n",
    "print 'median ranked binders:'\n",
    "mb = P.rankedBinders(name=name,cutoff=40)\n",
    "print mb\n",
    "\n",
    "ax=ep.utilities.venndiagram([pb.peptide, mb.peptide],\n",
    "                      ['promiscuous<5%','median rank<20'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "mpk_files = os.listdir(path_to_files+'netmhciipan/')\n",
    "P.data = pd.read_msgpack(mpk_files[0])\n",
    "P1.data = pd.read_msgpack(mpk_files[1])\n",
    "P2.data = pd.read_msgpack(mpk_files[2])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Deletion(variant=chr14.mpk'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpk_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-5bd4d239e6a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m \u001b[0mplot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplotStackedArea\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-5bd4d239e6a7>\u001b[0m in \u001b[0;36mplotStackedArea\u001b[0;34m(pred, title)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;31m#plot = Figure(plot_width=800, plot_height=400)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLength\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m     \u001b[0mseqlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/carlomazzaferro/anaconda/lib/python2.7/site-packages/epitopepredict/base.pyc\u001b[0m in \u001b[0;36mgetLength\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;34m\"\"\"Get peptide length of current set of predictions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeptide\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "import types\n",
    "\"\"\"\n",
    "colormaps={'tepitope':'Greens','netmhciipan':'Oranges','iedbmhc2':'Pinks',\n",
    "               'threading':'Purples','iedbmhc1':'Blues'}\n",
    "colors = {'tepitope':'green','netmhciipan':'orange',\n",
    "           'iedbmhc1':'blue','iedbmhc2':'pink','threading':'purple'}\n",
    "\n",
    "def plotTracks(predictors, title='', alleles=2, width=900, height=None,\n",
    "                seqdepot=None, bcell=None, exp=None, tools=True):\n",
    "        Plot binding predictions in multiple alleles for a single protein.\n",
    "        predictors: a dictionary of Predictor objects\n",
    "        with their predicted binder data usually for a single protein. If data from  \n",
    "        multiple proteins is provided the first one is used\n",
    "        alleles: the minimum number of alleles for a binder to be shown\n",
    "        \n",
    "\n",
    "    from collections import OrderedDict\n",
    "    from bokeh.palettes import Spectral3\n",
    "    \n",
    "    if type(predictors) is not types.ListType:\n",
    "        predictors = [predictors]\n",
    "    if tools == True:\n",
    "        tools=\"xpan, xwheel_zoom, resize, hover, reset, save\"\n",
    "    else:\n",
    "        tools=''\n",
    "     \n",
    "    #get title from the dataframe?\n",
    "    \n",
    "    alls=1\n",
    "    n = alleles\n",
    "    for p in predictors:\n",
    "        alls += len(p.data.groupby('allele'))\n",
    "    if height==None:\n",
    "        height = 130+10*alls\n",
    "    yrange = Range1d(start=0, end=alls+3)\n",
    "    plot = Figure(title=title,title_text_font_size=\"11pt\",plot_width=width,\n",
    "                  plot_height=height, y_range=yrange,\n",
    "                y_axis_label='allele',\n",
    "                tools=tools,\n",
    "                background_fill_color=\"#FAFAFA\",\n",
    "                toolbar_location=\"below\")\n",
    "    h=3\n",
    "    '''if bcell != None:\n",
    "        plotBCell(plot, bcell, alls)\n",
    "    if seqdepot != None:\n",
    "        plotAnnotations(plot,seqdepot)\n",
    "    if exp is not None:\n",
    "        plotExp(plot, exp)'''\n",
    "\n",
    "    #lists for hover data\n",
    "    #we plot all rects at once\n",
    "    x=[];y=[];allele=[];widths=[];clrs=[];peptide=[]\n",
    "    predictor=[];position=[];score=[];leg=[]\n",
    "    l=80\n",
    "    for pred in predictors:  \n",
    "        m = pred.name\n",
    "        print m, pred           \n",
    "        df = pred.data        \n",
    "        sckey = pred.scorekey\n",
    "        pb = pred.getPromiscuousBinders(data=df,n=n)\n",
    "        if len(pb) == 0:\n",
    "            continue\n",
    "        l = pred.getLength()\n",
    "        grps = df.groupby('allele')\n",
    "        alleles = grps.groups.keys()\n",
    "        if len(pb)==0:\n",
    "            continue\n",
    "        c=colors[m]\n",
    "        leg.append(m)\n",
    "\n",
    "        for a,g in grps:\n",
    "            b = pred.getBinders(data=g)             \n",
    "            b = b[b.pos.isin(pb.pos)] #only promiscuous\n",
    "            b.sort_values('pos',inplace=True)\n",
    "            scores = b[sckey].values\n",
    "            score.extend(scores)\n",
    "            pos = b['pos'].values\n",
    "            position.extend(pos)\n",
    "            x.extend(pos+(l/2.0)) #offset as coords are rect centers\n",
    "            widths.extend([l for i in scores])\n",
    "            clrs.extend([c for i in scores])\n",
    "            y.extend([h+0.5 for i in scores])\n",
    "            alls = [a for i in scores]\n",
    "            allele.extend(alls)\n",
    "            peptide.extend(list(b.peptide.values))\n",
    "            predictor.extend([m for i in scores])\n",
    "            h+=1\n",
    "\n",
    "    source = ColumnDataSource(data=dict(x=x,y=y,allele=allele,peptide=peptide,\n",
    "                                    predictor=predictor,position=position,score=score))\n",
    "    plot.rect(x,y, width=widths, height=0.8,\n",
    "         #x_range=Range1d(start=1, end=seqlen+l),\n",
    "         color=clrs,line_color='gray',alpha=0.7,source=source)\n",
    "    \n",
    "    hover = plot.select(dict(type=HoverTool))\n",
    "    hover.tooltips = OrderedDict([\n",
    "        (\"allele\", \"@allele\"),\n",
    "        (\"position\", \"@position\"),\n",
    "        (\"peptide\", \"@peptide\"),\n",
    "        (\"score\", \"@score\"),\n",
    "        (\"predictor\", \"@predictor\"),\n",
    "    ])\n",
    "\n",
    "    seqlen = pred.data.pos.max()+l\n",
    "    plot.set(x_range=Range1d(start=0, end=seqlen+1))#, bounds=(0, seqlen+1)))\n",
    "    plot.xaxis.major_label_text_font_size = \"8pt\"\n",
    "    plot.xaxis.major_label_text_font_style = \"bold\"\n",
    "    plot.ygrid.grid_line_color = None\n",
    "    plot.yaxis.major_label_text_font_size = '0pt'\n",
    "    plot.xaxis.major_label_orientation = np.pi/4        \n",
    "    return plot\n",
    "\n",
    "plot = plotTracks([P,P2],alleles=3)\n",
    "show(plot)\n",
    "\"\"\"\n",
    "        \n",
    "def plotStackedArea(pred, title=None):\n",
    "    from bokeh.charts import Area,Line\n",
    "    from bokeh.palettes import brewer, Spectral11\n",
    "    \n",
    "    tools=\"xpan, xwheel_zoom, resize, hover, reset, save\"\n",
    "    #plot = Figure(plot_width=800, plot_height=400)\n",
    "        \n",
    "    l = pred.getLength()        \n",
    "    seqlen = pred.data.pos.max()+l   \n",
    "    \n",
    "    if title == None:\n",
    "        title = list(pred.data.head(1).name)[0]\n",
    "    #calculate plot data\n",
    "    df = pred.data\n",
    "    #b = pred.getBinders(data=df,n=n)\n",
    "    #l = pred.getLength()\n",
    "    grps = df.groupby('allele')\n",
    "    #colors = brewer[\"Spectral\"][len(grps)]\n",
    "    #print t\n",
    "    data = {}\n",
    "    scores = []; pos=[]    \n",
    "    for i,g in grps:\n",
    "        #get running mean instead?        \n",
    "        y = g.sort_values('pos')[pred.scorekey]                    \n",
    "        y = y.clip(lower=0)\n",
    "        #y = pd.rolling_mean(y, window=l, center=True).fillna(0)\n",
    "        data[i] = y\n",
    "        scores.extend(y.values)\n",
    "        pos.extend(g.pos.values)\n",
    "    \n",
    "    #source = ColumnDataSource(data=dict(x=t.index,y=t.values))\n",
    "    p = Area(data, title=title, legend=\"top_left\", width=900, height=400, palette=Spectral11, \n",
    "                stack=True, xlabel='position', ylabel='score', tools=tools)\n",
    "    \n",
    "    grid = p.select(type=Grid)\n",
    "    grid.grid_line_color = None\n",
    "    p.set(x_range=Range1d(start=0, end=seqlen+1, bounds=(0, seqlen+1)))\n",
    "            \n",
    "    #glyphs = p.select(dict(type=GlyphRenderer))\n",
    "    hover = p.select(dict(type=HoverTool))\n",
    "    hover.tooltips = OrderedDict([\n",
    "        (\"x\", \"@x\"),\n",
    "        (\"y\", \"@y\"),\n",
    "        #(\"score\", \"@scores\"),\n",
    "    ])\n",
    "    return p\n",
    "\n",
    "plot=plotStackedArea(P)\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
