{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Introduction to variantannotation: package for the aggregation of genomic variant data \n",
    "\n",
    "#### Author: C. Mazzaferro & Kathleen Fisch\n",
    "#### Email: cmazzafe@ucsd.edu\n",
    "#### Date: June 2016\n",
    " \n",
    "## Outline of Notebook\n",
    "<a id = \"toc\"></a>\n",
    "1. <a href = \"#background\">Background</a>\n",
    "2. <a href = \"#setup\">Set Up File and Libraries</a>\n",
    "3. <a href = \"#ANNOVAR\">Run Annovar</a>\n",
    "4. <a href = \"#myvariant\">Obtain data from myvariant.info</a>\n",
    "5. <a href = \"#filter\">Variant Filtering & File Creation</a>\n",
    "    * <a href = \"#tumorvars\">Rare Tumor Variant Filter</a>\n",
    "    * <a href = \"#diseasevars\">Rare Diesease Variant Filter</a>\n",
    "    * <a href = \"#caddvars\">CADD PHRED High Impact Variants</a>\n",
    "    * <a href = \"#own\">Create Your Own Filter</a>\n",
    "6. <a href = \"#export\">Export CSV and VCF files</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"background\"></a>\n",
    "## Background\n",
    "\n",
    "This notebook will walk you through the steps of how variants coming from a VCF can be annotated efficiently and thoroughly using the package variantannotation. In particular, the package is aimed at providing a way of retrieving variant information using [ANNOVAR](http://annovar.openbioinformatics.org/en/latest/) and [myvariant.info](myvariant.info) and consolidating it in conveninent formats. It is well-suited for bioinformaticians interested in aggregating variant information into a single database for ease of use and to provide higher analysis capabities. \n",
    "\n",
    "The aggregation is performed specifically by structuring the data in lists of python dictionaries, with each variant being described by a multi-level dictionary. The choice was made due to the inconsistencies that exist between data availability, and the necessity to store their information in a flexible manner. Further, this specific format permits its parsing to a MongoDb instance (dictionaries are the python representation of JSON objects), which enables the user to efficiently store, query and filter such data. \n",
    "\n",
    "Finally, the package also has the added functionality to create csv and vcf files from MongoDB. The class Filters allows the user to rapidly query data that meets certain criteria as a list of documents, and the class FileWriter can transform such list into more widely accepted formats such as vcf and csv files. It should be noted that here, the main differential the package offers is the ability to write these files preserving all the annotation data. In the vcf files, for instance, outputs will have a 'Otherinfo' column where all the data coming from ANNOVAR and myvariant.info is condensed (while still preserving its structure). For vcf files, outputs will have around ~120-200 columns, depending on the amount of variant data that can be retrieved from myvvariant.info. \n",
    "\n",
    "\n",
    "**Notes on required software**\n",
    "\n",
    "the following libraries will be install upon installing variantannotation:\n",
    "- myvariant\n",
    "- pysam\n",
    "- pymongo\n",
    "- pyvcf\n",
    "\n",
    "Other libraries that are needed, but should natively e installed on most OS: \n",
    "\n",
    "- Watchdog\n",
    "- Pandas\n",
    "- Numpy\n",
    "\n",
    "Further, a MongoDB database must be set up. Refer to the documentation page for more information. \n",
    "Similarly, ANNOVAR must be downloaded, alongside with its supporting databases (also listed on the documentation page)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"setup\"></a>\n",
    "## Import libraries and specify file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlomazzaferro/anaconda/lib/python2.7/site-packages/matplotlib/__init__.py:1035: UserWarning: Duplicate key in file \"/Users/carlomazzaferro/.matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import vcf\n",
    "import time\n",
    "import pysam\n",
    "import myvariant\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "#variantannotation functions\n",
    "from variantannotation import annotate_batch\n",
    "from variantannotation import create_output_files\n",
    "from variantannotation import myvariant_parsing_utils\n",
    "from variantannotation import mongo_DB_export\n",
    "from variantannotation import utilities\n",
    "from variantannotation import MongoDB_querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ANNOVAR_PATH = '/data/annovar/'\n",
    "FILE_NAMES = ['Tumor_RNAseq_variants.vcf', 'Tumor_targeted_seq.vcf', 'normal_targeted_seq.vcf', 'normal_blood_WGS.vqsr.vcf', 'somatic_mutect_old.vcf']\n",
    "IN_PATH = '/data/ccbb_internal/interns/Carlo/test_vcf/'\n",
    "OUT_PATH = '/data/ccbb_internal/interns/Carlo/test_vcf_out/'\n",
    "vcf_file = IN_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/ccbb_internal/interns/Carlo/test_vcf/Tumor_RNAseq_variants.vcf\n",
      "/data/ccbb_internal/interns/Carlo/test_vcf/Tumor_targeted_seq.vcf\n",
      "/data/ccbb_internal/interns/Carlo/test_vcf/normal_targeted_seq.vcf\n",
      "/data/ccbb_internal/interns/Carlo/test_vcf/normal_blood_WGS.vqsr.vcf\n",
      "/data/ccbb_internal/interns/Carlo/test_vcf/somatic_mutect_old.vcf\n"
     ]
    }
   ],
   "source": [
    "#Check if file paths are correctly pointing to the specified files.\n",
    "for i in range(0, len(FILE_NAMES)):\n",
    "    print IN_PATH+FILE_NAMES[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"ANNOVAR\"></a>\n",
    "## Run Annovar \n",
    "\n",
    "This will run ANNOVAR. A csv file named tumortargcsvout.hg19_multianno.csv will appear in the OUT_PATH specified. The csv file can then be processed and integrated with the data coming from myvariant.info. \n",
    "This command may take a some time to run (5-30 minutes for each file depending on file size).\n",
    "To keep things simple, we can start by looking at one file only. Let's run annovar on it. In any case, if you have multiple files to work on, you can run them in parallel by running the block after the next one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently working on VCF file: Tumor_RNAseq_variants, field avinput\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field variant_function\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field exonic_variant_function\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_tfbsConsSites\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_cytoBand\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_targetScanS\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_genomicSuperDups\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_gwasCatalog\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_esp6500siv2_all_filtered\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_esp6500siv2_all_dropped\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field 2015_08_filtered\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field 2015_08_dropped\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_snp138_filtered\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_snp138_dropped\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_ljb26_all_filtered\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_ljb26_all_dropped\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_cg46_filtered\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_cg46_dropped\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_cg69_filtered\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_cg69_dropped\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_popfreq_all_filtered\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_popfreq_all_dropped\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_clinvar_20140929_filtered\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_clinvar_20140929_dropped\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_cosmic70_filtered\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_cosmic70_dropped\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_nci60_filtered\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_nci60_dropped\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field csv\n",
      "\n",
      "Annovar finished working on file : Tumor_RNAseq_variants has finished. A .csv file has been created in the \n",
      " OUT_PATH directory\n",
      "Currently working on VCF file: Tumor_targeted_seq, field avinput\n",
      "Currently working on VCF file: Tumor_targeted_seq, field variant_function\n",
      "Currently working on VCF file: Tumor_targeted_seq, field exonic_variant_function\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_tfbsConsSites\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_cytoBand\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_targetScanS\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_genomicSuperDups\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_gwasCatalog\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_esp6500siv2_all_filtered\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_esp6500siv2_all_dropped\n",
      "Currently working on VCF file: Tumor_targeted_seq, field 2015_08_filtered\n",
      "Currently working on VCF file: Tumor_targeted_seq, field 2015_08_dropped\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_snp138_filtered\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_snp138_dropped\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_ljb26_all_filtered\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_ljb26_all_dropped\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_cg46_filtered\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_cg46_dropped\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_cg69_filtered\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_cg69_dropped\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_popfreq_all_filtered\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_popfreq_all_dropped\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_clinvar_20140929_filtered\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_clinvar_20140929_dropped\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_cosmic70_filtered\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_cosmic70_dropped\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_nci60_filtered\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_nci60_dropped\n",
      "Currently working on VCF file: Tumor_targeted_seq, field csv\n",
      "\n",
      "Annovar finished working on file : Tumor_targeted_seq has finished. A .csv file has been created in the \n",
      " OUT_PATH directory\n",
      "Currently working on VCF file: normal_targeted_seq, field avinput\n",
      "Currently working on VCF file: normal_targeted_seq, field variant_function\n",
      "Currently working on VCF file: normal_targeted_seq, field exonic_variant_function\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_tfbsConsSites\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_cytoBand\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_targetScanS\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_genomicSuperDups\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_gwasCatalog\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_esp6500siv2_all_filtered\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_esp6500siv2_all_dropped\n",
      "Currently working on VCF file: normal_targeted_seq, field 2015_08_filtered\n",
      "Currently working on VCF file: normal_targeted_seq, field 2015_08_dropped\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_snp138_filtered\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_snp138_dropped\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_ljb26_all_filtered\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_ljb26_all_dropped\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_cg46_filtered\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_cg46_dropped\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_cg69_filtered\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_cg69_dropped\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_popfreq_all_filtered\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_popfreq_all_dropped\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_clinvar_20140929_filtered\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_clinvar_20140929_dropped\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_cosmic70_filtered\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_cosmic70_dropped\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_nci60_filtered\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_nci60_dropped\n",
      "Currently working on VCF file: normal_targeted_seq, field csv\n",
      "\n",
      "Annovar finished working on file : normal_targeted_seq has finished. A .csv file has been created in the \n",
      " OUT_PATH directory\n",
      "Currently working on VCF file: normal_blood_WGS, field avinput\n",
      "Currently working on VCF file: normal_blood_WGS, field variant_function\n",
      "Currently working on VCF file: normal_blood_WGS, field exonic_variant_function\n",
      "Currently working on VCF file: normal_blood_WGS, field hg19_tfbsConsSites\n",
      "Currently working on VCF file: normal_blood_WGS, field hg19_cytoBand\n",
      "Currently working on VCF file: normal_blood_WGS, field hg19_targetScanS\n",
      "Currently working on VCF file: normal_blood_WGS, field hg19_genomicSuperDups\n",
      "Currently working on VCF file: normal_blood_WGS, field hg19_gwasCatalog\n",
      "Currently working on VCF file: normal_blood_WGS, field hg19_esp6500siv2_all_filtered\n",
      "Currently working on VCF file: normal_blood_WGS, field hg19_esp6500siv2_all_dropped\n",
      "Currently working on VCF file: normal_blood_WGS, field 2015_08_filtered\n",
      "Currently working on VCF file: normal_blood_WGS, field 2015_08_dropped\n",
      "Currently working on VCF file: normal_blood_WGS, field hg19_snp138_filtered\n",
      "Currently working on VCF file: normal_blood_WGS, field hg19_snp138_dropped\n"
     ]
    }
   ],
   "source": [
    "utilities.run_annovar(ANNOVAR_PATH, IN_PATH+FILE_NAMES[0], OUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Annovar runs as a subprocess on every file. They will run in parallel for speed up. \n",
    "for i in range(0, len(FILE_NAMES)):\n",
    "    utilities.run_annovar(ANNOVAR_PATH, IN_PATH+FILE_NAMES[i], OUT_PATH)\n",
    "\n",
    "#This serves to give a real-time feedback of the ANNOVAR progress and status. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the name and location of the csv file that ANNOVAR produces as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "filepath_out = '/data/ccbb_internal/interns/Carlo/test_vcf_out/'\n",
    "filepath_in = '/data/ccbb_internal/interns/Carlo/test_vcf/'\n",
    "\n",
    "#For safety, check the files in directory. Either run '!ls' here on iPython, or go to the directory and check \n",
    "#manually for existing files. There should be once csv file for every vcf file. \n",
    "\n",
    "VCF_FILE_NAME = 'Tumor_RNAseq_variants.vcf'\n",
    "CSV_FILE_NAME = 'Tumor_RNAseq_variants.hg19_multianno.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"myvariant\"></a>\n",
    "## Getting data from myvariant.info\n",
    "The package offers 4 different methods to obtain variant data. Two of them require annovar, while the other two are based solely on the use of myvariant.info service. The latter can be used without having to worry about downloading and installing annovar databases, but it tends to return partial or no information for some variants. \n",
    "\n",
    "The different methods also enable the user to decide how the data will be parsed to MongoDB. 1 and 3 parse the data by chunks: the user specifies a number of variants (usually 1000), and the data from the vcf and csv files are parsed as soon as those 1000 variants are processed and integrated. This enables huge files to be processed without having to hold them in memory and potentially cause a Memory Overflow error. \n",
    "\n",
    "Methods 2 and 4, on the other hand, process the files on their entirety and send them to MongoDB at once. Well-suited for smaller files. See docs for more info. \n",
    "\n",
    "## Export data to MongoDB by chunks, iteratively. \n",
    "\n",
    "For this tutorial, we will use method #1. Data from annovar (as a csv file) will be obtained 1000 lines at a time, instead of attempting to parse and process an entire csv file at once.\n",
    "\n",
    "As soon as you run the scripts from variantannotaiton, variant data will be retrieved from myvariant.info and the data will automatically be integrated and stored to MongoDB. Database and collection name should be specified, and there must be a running MongoDB connection. The script will set up a client to communicate between python (through pymongo) and the the database.\n",
    "\n",
    "In general, the shell command:\n",
    "\n",
    "`mongod --dbpath ../data/db`  \n",
    "\n",
    "where data/db is the designated location where the data will be stored, will initiate MongoDB. After this, the script should store data to the directory automatically.\n",
    "For pymongo, and more information on how to set up a Mongo Database: https://docs.mongodb.com/getting-started/python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 1 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 2 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 3 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 4 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 5 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 6 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 7 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 8 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 9 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 10 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 11 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 12 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 13 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 14 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 15 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 16 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 17 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 18 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 19 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 20 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 21 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 22 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 23 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 24 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 25 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 26 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 27 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 28 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 29 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 30 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 31 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 32 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 33 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 34 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 35 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 36 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 37 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 38 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 39 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 40 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 41 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 42 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-2710...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 43 of 43\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished!'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunksize = 10000\n",
    "step = 0\n",
    "\n",
    "#Get variant list. Should always be the first step after running ANNOVAR\n",
    "open_file = myvariant_parsing_utils.VariantParsing()\n",
    "\n",
    "#Name Collections & DB. Change them to something appropriate. Each file should live in a collection\n",
    "db_name = 'Variant_Prioritization_Workflow'\n",
    "\n",
    "collection_name = 'Test_Tumor_RNAseq'\n",
    "\n",
    "list_file = open_file.get_variants_from_vcf(filepath_in+VCF_FILE_NAME)\n",
    "as_batch = annotate_batch.AnnotationMethods()\n",
    "as_batch.by_chunks(list_file, chunksize, step, filepath_out+CSV_FILE_NAME, collection_name, db_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"filter\"></a>\n",
    "## Variant Filtering & Output Files\n",
    "\n",
    "Here we implement three different filters that allow for the retrieval of specific variants. The filters are implemented as MongoDB queries, and are designed to provie the user with a set of relevant variants. In case the user would like to define its own querying, a template is provided. \n",
    "The output of the queries is a list of dictionaries (JSON documents), where each dictionary contains data reltive to one variant. \n",
    "\n",
    "Further, the package allows the user to parse these variants into an annotated csv or vcf file. \n",
    "If needed, annotated, unfiltered vcf and csv files can also be created. They will have the same length (number of variants) as the original files, but will contain much more complete annotation data coming from myvariant.info and ANNOVAR databases. \n",
    "\n",
    "To create a csv file, just the filtered output is needed. To create an annotated vcf file, a tab indexed file (.tbi) file is needed (see comments in  section Create unfiltered annotated vcf and csv files at the end of this page). This can be created using tabix.  \n",
    "\n",
    "First, the file needs to be compressed:\n",
    "\n",
    "From the command line, running:\n",
    "\n",
    "`bgzip -c input_file.vcf > input_file.vcf.gz`\n",
    "\n",
    "returns `input_vcf_file.vcf.gz`\n",
    "\n",
    "and running \n",
    "\n",
    "`tabix input_vcf_file.vcf.gz`\n",
    "\n",
    "will return: `input_vcf_file.vcf.gz.tbi`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"tumorvars\"></a>\n",
    "## Filter #1: specifying cancer-specifc rare variants\n",
    "\n",
    " - filter 1: ThousandGenomeAll < 0.05 or info not available\n",
    " - filter 2: ESP6500siv2_all < 0.05 or info not available\n",
    " - filter 3: cosmic70 information is present\n",
    " - filter 4: Func_knownGene is exonic, splicing, or both\n",
    " - filter 5: ExonicFunc_knownGene is not \"synonymous SNV\"\n",
    " - filter 6: Read Depth (DP) > 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filepath = '/data/ccbb_internal/interns/Carlo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variants found that match rarity criteria: 11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished writing annotated, filtered VCF file'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create output files (if needed): specify name of files and path \n",
    "rare_cancer_variants_csv = filepath + \"/tumor_rna_rare_cancer_vars_csv.csv\"\n",
    "rare_cancer_variants_vcf = filepath + \"/tumor_rna_rare_cancer_vars_vcf.vcf\"\n",
    "input_vcf_compressed = filepath + '/test_vcf/Tumor_RNAseq_variants.vcf.gz'\n",
    "\n",
    "#Apply filter.\n",
    "filter_collection = MongoDB_querying.Filters(db_name, collection_name)\n",
    "rare_cancer_variants = filter_collection.rare_cancer_variant()\n",
    "\n",
    "#Crete writer object for filtered lists:\n",
    "my_writer = create_output_files.FileWriter(db_name, collection_name)\n",
    "\n",
    "#cancer variants filtered files\n",
    "my_writer.generate_annotated_csv(rare_cancer_variants, rare_cancer_variants_csv)\n",
    "my_writer.generate_annotated_vcf(rare_cancer_variants,input_vcf_compressed, rare_cancer_variants_vcf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"rarevars\"></a>\n",
    "## Filter #2: specifying rare disease-specifc (rare) variants\n",
    "\n",
    "- filter 1: ThousandGenomeAll < 0.05 or info not available\n",
    "- filter 2: ESP6500siv2_all < 0.05 or info not available\n",
    "- filter 3: cosmic70 information is present\n",
    "- filter 4: Func_knownGene is exonic, splicing, or both\n",
    "- filter 5: ExonicFunc_knownGene is not \"synonymous SNV\"\n",
    "- filter 6: Read Depth (DP) > 10\n",
    "- filter 7: Clinvar data is present "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variants found that match rarity criteria: 0\n"
     ]
    }
   ],
   "source": [
    "#Apply filter.\n",
    "filter_collection = MongoDB_querying.Filters(db_name, collection_name)\n",
    "rare_disease_variants = filter_collection.rare_disease_variant()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero variants found. Writing a csv output won't make much sense. You can still customize the filters the way you'd like, as you can see below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"own\"></a>\n",
    "## Create your own filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As long as you have a MongoDB instance running, filtering can be perfomed trough pymongo as shown by the code below. If a list is intended to be created from it, simply add: `filter2 = list(filter2)`\n",
    "\n",
    "If you'd like to customize your filters, a good idea would be to look at the available fields to be filtered. Looking at the myvariant.info [documentation](http://docs.myvariant.info/en/latest/doc/data.html), you can see what are all the fields avaialble and can be used for filtering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient()\n",
    "db = client.My_Variant_Database\n",
    "collection = db.ANNOVAR_MyVariant_chunks\n",
    "\n",
    "filter2 = collection.find({ \"$and\": [\n",
    "                                 {\"$or\": [{\"ThousandGenomeAll\": {\"$lt\": 0.05}}, {\"ThousandGenomeAll\": {\"$exists\": False}}]},\n",
    "                                 {\"$or\": [{\"ESP6500siv2_all\": { \"$lt\": 0.05}}, {\"ESP6500siv2_all\": { \"$exists\": False}}]},\n",
    "                                 {\"$or\": [{\"Func_knownGene\": \"exonic\"}, {\"Func_knownGene\": \"splicing\"}]},\n",
    "                                 {\"ExonicFunc_knownGene\": {\"$ne\": \"synonymous SNV\"}},\n",
    "                                 {\"Genotype_Call.DP\": {\"$gte\": 10}},\n",
    "                                 {\"cosmic70\": { \"$exists\": True}}\n",
    "                         ]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"export\"></a>\n",
    "## Create unfiltered annotated vcf and csv files \n",
    "Let's write an output file that contains all annotation data. This may be useful for researchers interested in obtaining a full description of their files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Finished writing annotated VCF file'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create output files (if needed): specify name of files and path \n",
    "out_unfiltered_vcf_file = filepath + \"/out_unfiltered_rnaseq_vcf.vcf\"\n",
    "out_unfiltered_csv_file = filepath + \"/out_unfiltered_rnaseq_csv.csv\"\n",
    "input_vcf_compressed = filepath + '/test_vcf/Tumor_RNAseq_variants.vcf.gz'\n",
    "\n",
    "#Create writer object\n",
    "#db and collection name must be specified since no list is given. The entire collection will be queried. \n",
    "my_writer = create_output_files.FileWriter(db_name, collection_name)\n",
    "\n",
    "#Write collection to csv and vcf\n",
    "#The in_vcf_file must be the .vcf.gz file and it needs to have an associated .tbi file. \n",
    "\n",
    "\n",
    "my_writer.generate_unfiltered_annotated_csv(out_unfiltered_csv_file)\n",
    "my_writer.generate_unfiltered_annotated_vcf(input_vcf_compressed, out_unfiltered_vcf_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do some cool downstream analysis on your newly created datasets :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read filtered vcf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from VarP import utils\n",
    "\n",
    "reader_names = ['Tumor_RNA_Reader', 'Tumor_Targeted_Reader',\n",
    "           'Normal_DNA_Reader', 'Normal_Blood_Reader',\n",
    "           'Somatic_Mutect_Reader']   #bug fixed in source\n",
    "\n",
    "path_to_files ='/Volumes/Seagate Backup Plus Drive/vcf_files/varcode_to_test/' \n",
    "myhandler = utils.HandleReaders(reader_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "### Create a list of variant collections. See varcode's documentation for more info on this type of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-875c2111fbbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist_collections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_collection_from_readers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_collections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Let's pick one of the collections to start working with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmy_collection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist_collections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/carlomazzaferro/Documents/CCBB/neoantigen/VarP-master/VarP/utils.py\u001b[0m in \u001b[0;36mcreate_collection_from_readers\u001b[0;34m(self, path_to_dir)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".vcf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_vcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                 \u001b[0mreader_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": [
    "list_collections = myhandler.create_collection_from_readers(path_to_files)\n",
    "\n",
    "print type(list_collections[4])\n",
    "#Let's pick one of the collections to start working with\n",
    "my_collection = list_collections[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create desired outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_coding_effects = myhandler.return_list_coding_effects(my_collection)   #list of coding effects\n",
    "protein_list = myhandler.return_protein_list(list_coding_effects)           #list of proteins\n",
    "dataframe = myhandler.return_dataframe(protein_list, list_coding_effects)   #dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Fasta file\n",
    "myhandler.generate_fasta_file(dataframe, path_to_files+'PEPTIDES.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Epitope Prediction\n",
    "#### Use epitope predict and netMHCpan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'netMHCIIpan -s -length 11 -a DRB1_0101 -f tempseq.fa' returned non-zero exit status 127",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-ef44fc0014f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m            \"HLA-DRB1*0404\", \"HLA-DRB3*0101\", \"HLA-DRB4*0104\"]\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictProteins\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzaire\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malleles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malleles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msavepath1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/root/anaconda3/envs/py27/lib/python2.7/site-packages/epitopepredict/base.pyc\u001b[0m in \u001b[0;36mpredictProteins\u001b[1;34m(self, recs, length, names, alleles, save, label, path)\u001b[0m\n\u001b[0;32m    547\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0malleles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m                 df = self.predict(sequence=seq,length=length,\n\u001b[1;32m--> 549\u001b[1;33m                                     allele=a,name=name)\n\u001b[0m\u001b[0;32m    550\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m                     \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda3/envs/py27/lib/python2.7/site-packages/epitopepredict/base.pyc\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, sequence, peptides, length, allele, name, pseudosequence)\u001b[0m\n\u001b[0;32m    813\u001b[0m                 \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    814\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 815\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunSequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallele\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    816\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda3/envs/py27/lib/python2.7/site-packages/epitopepredict/base.pyc\u001b[0m in \u001b[0;36mrunSequence\u001b[1;34m(self, seq, length, allele)\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mcmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'netMHCIIpan -s -length %s -a %s -f %s'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallele\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseqfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m         \u001b[1;31m#print cmd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 793\u001b[1;33m         \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'/bin/bash'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    794\u001b[0m         \u001b[0mrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    795\u001b[0m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda3/envs/py27/lib/python2.7/subprocess.pyc\u001b[0m in \u001b[0;36mcheck_output\u001b[1;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcmd\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m             \u001b[0mcmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpopenargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 573\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    574\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mCalledProcessError\u001b[0m: Command 'netMHCIIpan -s -length 11 -a DRB1_0101 -f tempseq.fa' returned non-zero exit status 127"
     ]
    }
   ],
   "source": [
    "from epitopepredict import base, sequtils, analysis\n",
    "\n",
    "sns.set_context(\"notebook\", font_scale=1.4)\n",
    "fastafile =  path_to_files+'PEPTIDES.txt'\n",
    "\n",
    "#get data in DF format\n",
    "zaire = sequtils.fasta2Dataframe(fastafile)\n",
    "\n",
    "P = base.getPredictor('netmhciipan')\n",
    "savepath1 = 'netmhciipan'\n",
    "#run prediction for several alleles and save results to savepath\n",
    "alleles = [\"HLA-DRB1*0101\", \"HLA-DRB1*0108\", \"HLA-DRB1*0305\", \"HLA-DRB1*0401\",\n",
    "           \"HLA-DRB1*0404\", \"HLA-DRB3*0101\", \"HLA-DRB4*0104\"]\n",
    "\n",
    "P.predictProteins(zaire, length=11, alleles=alleles, save=True, path=savepath1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'tepitope/ZEBOVgp5.mpk'\n",
    "filename2 = 'iedbmhc1/ZEBOVgp5.mpk'\n",
    "P.data = pd.read_msgpack(filename)\n",
    "P2.data = pd.read_msgpack(filename2)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
