{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Variant Annotation, Filtering, Prioritization\n",
    "## Sample usage of variantannotation and VarP packages as a pipeline for new epitope prediction\n",
    "\n",
    "#### Author: C. Mazzaferro, K. Fisch\n",
    "#### Email: cmazzafe@ucsd.edu\n",
    "#### Date: August 2016\n",
    " \n",
    "## Outline of Notebook\n",
    "<a id = \"toc\"></a>\n",
    "1. <a href = \"#background\">Background</a>\n",
    "2. <a href = \"#setup\">Set Up File and Libraries</a>\n",
    "3. <a href = \"#ANNOVAR\">Run Annovar</a>\n",
    "4. <a href = \"#myvariant\">Obtain data from myvariant.info</a>\n",
    "5. <a href = \"#filter\">Variant Filtering & File Creation</a>\n",
    "    * <a href = \"#tumorvars\">Rare Tumor Variant Filter</a>\n",
    "6. <a href = \"#VarP\">Variant Prioritization</a>\n",
    "    * <a href = \"#out_fasta_df\">Create Fasta File and Dataframe</a>\n",
    "7. <a href = \"#ep_pred\">New Epitope Prediction</a>\n",
    "    * <a href = \"#clustering\">Epitope Clustering Analysis</a>\n",
    "    * <a href = \"#conservation\">Epitope Conservation Analysis</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"background\"></a>\n",
    "## Background\n",
    "\n",
    "This notebook will walk you through the steps of how variants coming from a VCF can be annotated, filtered, prioritized, and finally analyzed by their effect on protein binging prediction and likelihood of being mutant epitopes. The interest of this notebook is providing a tool to bionformaticians to enhance reproducibility in NGS data analysis. \n",
    "\n",
    "## Application of Framework to an N-of-1 Study\n",
    "\n",
    "As a case study, we have applied this framework to create a molecular profile of uveal melanoma for neoantigen discovery in an N-of-1 study. Germline whole genome sequencing (WGS), tumor RNA sequencing (RNAseq) and targeted tumor sequencing data were analyzed, integrated and interpreted. We calculated relative gene expression, detected transcribed, germline and somatic mutations, determined HLA types and expression, and predicted mutant epitopes. All raw data were aligned to the human genome (hg19), variants were called using the GATK best practices for DNA and RNA, gene expression was quantified using RSEM, and normalized with DESEQ2. TCGA UVM RNAseqV2 raw count data were used as a reference cohort for gene expression. Genes from the DecisionDX-UM gene panel from the patient and TCGA were clustered using hierarchical clustering for subtype discovery. Variants from all datasets were overlaid in Integrative Genomics Viewer (IGV) to identify somatic mutations with high confidence. HLA types and expression were calculated with seq2HLA. Mutant epitopes were predicted using Varcode and MHC binding prediction was performed with NetMHCcons.  The N of 1 design of this study provided an interesting perspective on data interpretation and visualization and provided the patient and clinicians with a genomic profile of the tumor. This study highlights the need for reproducible, open-source biological interpretation tools, such as Jupyter-Genomics, to further translational medicine. We hope to enhance transparency and reproducibility of next generation sequencing analysis and biological interpretation, which will be needed for translating this technology into routine clinical practice.\n",
    "\n",
    "\n",
    "*This workflow solely requires as input a vcf file. Annovar and parsing to MongoDB are handled by the functions defined in the package\n",
    "\n",
    "**Notes on required software**\n",
    "\n",
    "the following libraries will be install upon installing variantannotation:\n",
    "- myvariant\n",
    "- pysam\n",
    "- pymongo\n",
    "- pyvcf\n",
    "\n",
    "Other libraries that are needed, but should natively e installed on most OS: \n",
    "\n",
    "- Watchdog\n",
    "- Pandas\n",
    "- Numpy\n",
    "\n",
    "Further, a MongoDB database must be set up. Refer to the documentation page for more information. \n",
    "Similarly, ANNOVAR must be downloaded, alongside with its supporting databases (also listed on variantannotation's the documentation page)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"setup\"></a>\n",
    "## Import libraries and specify file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import vcf\n",
    "import time\n",
    "import pysam\n",
    "import myvariant\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "#variantannotation functions\n",
    "from variantannotation import annotate_batch\n",
    "from variantannotation import create_output_files\n",
    "from variantannotation import myvariant_parsing_utils\n",
    "from variantannotation import mongo_DB_export\n",
    "from variantannotation import utilities\n",
    "from variantannotation import MongoDB_querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ANNOVAR_PATH = '/data/annovar/'\n",
    "FILE_NAMES = ['Tumor_RNAseq_variants.vcf', 'Tumor_targeted_seq.vcf', 'normal_targeted_seq.vcf', 'normal_blood_WGS.vqsr.vcf', 'somatic_mutect_old.vcf']\n",
    "IN_PATH = '/data/ccbb_internal/interns/Carlo/test_vcf/'\n",
    "OUT_PATH = '/data/ccbb_internal/interns/Carlo/test_vcf_out/'\n",
    "vcf_file = IN_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/ccbb_internal/interns/Carlo/test_vcf/Tumor_RNAseq_variants.vcf\n",
      "/data/ccbb_internal/interns/Carlo/test_vcf/Tumor_targeted_seq.vcf\n",
      "/data/ccbb_internal/interns/Carlo/test_vcf/normal_targeted_seq.vcf\n",
      "/data/ccbb_internal/interns/Carlo/test_vcf/normal_blood_WGS.vqsr.vcf\n",
      "/data/ccbb_internal/interns/Carlo/test_vcf/somatic_mutect_old.vcf\n"
     ]
    }
   ],
   "source": [
    "#Check if file paths are correctly pointing to the specified files.\n",
    "for i in range(0, len(FILE_NAMES)):\n",
    "    print IN_PATH+FILE_NAMES[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"ANNOVAR\"></a>\n",
    "## Run Annovar \n",
    "\n",
    "This will run ANNOVAR. A csv file named tumortargcsvout.hg19_multianno.csv will appear in the OUT_PATH specified. The csv file can then be processed and integrated with the data coming from myvariant.info. \n",
    "This command may take a some time to run (5-30 minutes for each file depending on file size).\n",
    "To keep things simple, we can start by looking at one file only. Let's run annovar on it. In any case, if you have multiple files to work on, you can run them in parallel by running the block after the next one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently working on VCF file: Tumor_RNAseq_variants, field avinput\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field variant_function\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field exonic_variant_function\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_tfbsConsSites\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_cytoBand\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_targetScanS\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_genomicSuperDups\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_gwasCatalog\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_esp6500siv2_all_filtered\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_esp6500siv2_all_dropped\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field 2015_08_filtered\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field 2015_08_dropped\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_snp138_filtered\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_snp138_dropped\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_ljb26_all_filtered\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_ljb26_all_dropped\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_cg46_filtered\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_cg46_dropped\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_cg69_filtered\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_cg69_dropped\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_popfreq_all_filtered\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_popfreq_all_dropped\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_clinvar_20140929_filtered\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_clinvar_20140929_dropped\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_cosmic70_filtered\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_cosmic70_dropped\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_nci60_filtered\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field hg19_nci60_dropped\n",
      "Currently working on VCF file: Tumor_RNAseq_variants, field csv\n",
      "\n",
      "Annovar finished working on file : Tumor_RNAseq_variants has finished. A .csv file has been created in the \n",
      " OUT_PATH directory\n",
      "Currently working on VCF file: Tumor_targeted_seq, field avinput\n",
      "Currently working on VCF file: Tumor_targeted_seq, field variant_function\n",
      "Currently working on VCF file: Tumor_targeted_seq, field exonic_variant_function\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_tfbsConsSites\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_cytoBand\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_targetScanS\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_genomicSuperDups\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_gwasCatalog\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_esp6500siv2_all_filtered\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_esp6500siv2_all_dropped\n",
      "Currently working on VCF file: Tumor_targeted_seq, field 2015_08_filtered\n",
      "Currently working on VCF file: Tumor_targeted_seq, field 2015_08_dropped\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_snp138_filtered\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_snp138_dropped\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_ljb26_all_filtered\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_ljb26_all_dropped\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_cg46_filtered\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_cg46_dropped\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_cg69_filtered\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_cg69_dropped\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_popfreq_all_filtered\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_popfreq_all_dropped\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_clinvar_20140929_filtered\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_clinvar_20140929_dropped\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_cosmic70_filtered\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_cosmic70_dropped\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_nci60_filtered\n",
      "Currently working on VCF file: Tumor_targeted_seq, field hg19_nci60_dropped\n",
      "Currently working on VCF file: Tumor_targeted_seq, field csv\n",
      "\n",
      "Annovar finished working on file : Tumor_targeted_seq has finished. A .csv file has been created in the \n",
      " OUT_PATH directory\n",
      "Currently working on VCF file: normal_targeted_seq, field avinput\n",
      "Currently working on VCF file: normal_targeted_seq, field variant_function\n",
      "Currently working on VCF file: normal_targeted_seq, field exonic_variant_function\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_tfbsConsSites\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_cytoBand\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_targetScanS\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_genomicSuperDups\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_gwasCatalog\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_esp6500siv2_all_filtered\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_esp6500siv2_all_dropped\n",
      "Currently working on VCF file: normal_targeted_seq, field 2015_08_filtered\n",
      "Currently working on VCF file: normal_targeted_seq, field 2015_08_dropped\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_snp138_filtered\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_snp138_dropped\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_ljb26_all_filtered\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_ljb26_all_dropped\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_cg46_filtered\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_cg46_dropped\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_cg69_filtered\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_cg69_dropped\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_popfreq_all_filtered\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_popfreq_all_dropped\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_clinvar_20140929_filtered\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_clinvar_20140929_dropped\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_cosmic70_filtered\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_cosmic70_dropped\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_nci60_filtered\n",
      "Currently working on VCF file: normal_targeted_seq, field hg19_nci60_dropped\n",
      "Currently working on VCF file: normal_targeted_seq, field csv\n",
      "\n",
      "Annovar finished working on file : normal_targeted_seq has finished. A .csv file has been created in the \n",
      " OUT_PATH directory\n",
      "Currently working on VCF file: normal_blood_WGS, field avinput\n",
      "Currently working on VCF file: normal_blood_WGS, field variant_function\n",
      "Currently working on VCF file: normal_blood_WGS, field exonic_variant_function\n",
      "Currently working on VCF file: normal_blood_WGS, field hg19_tfbsConsSites\n",
      "Currently working on VCF file: normal_blood_WGS, field hg19_cytoBand\n",
      "Currently working on VCF file: normal_blood_WGS, field hg19_targetScanS\n",
      "Currently working on VCF file: normal_blood_WGS, field hg19_genomicSuperDups\n",
      "Currently working on VCF file: normal_blood_WGS, field hg19_gwasCatalog\n",
      "Currently working on VCF file: normal_blood_WGS, field hg19_esp6500siv2_all_filtered\n",
      "Currently working on VCF file: normal_blood_WGS, field hg19_esp6500siv2_all_dropped\n",
      "Currently working on VCF file: normal_blood_WGS, field 2015_08_filtered\n",
      "Currently working on VCF file: normal_blood_WGS, field 2015_08_dropped\n",
      "Currently working on VCF file: normal_blood_WGS, field hg19_snp138_filtered\n",
      "Currently working on VCF file: normal_blood_WGS, field hg19_snp138_dropped\n"
     ]
    }
   ],
   "source": [
    "utilities.run_annovar(ANNOVAR_PATH, IN_PATH+FILE_NAMES[0], OUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Annovar runs as a subprocess on every file. They will run in parallel for speed up. \n",
    "for i in range(0, len(FILE_NAMES)):\n",
    "    utilities.run_annovar(ANNOVAR_PATH, IN_PATH+FILE_NAMES[i], OUT_PATH)\n",
    "\n",
    "#This serves to give a real-time feedback of the ANNOVAR progress and status. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the name and location of the csv file that ANNOVAR produces as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "filepath_out = '/data/ccbb_internal/interns/Carlo/test_vcf_out/'\n",
    "filepath_in = '/data/ccbb_internal/interns/Carlo/test_vcf/'\n",
    "\n",
    "#For safety, check the files in directory. Either run '!ls' here on iPython, or go to the directory and check \n",
    "#manually for existing files. There should be once csv file for every vcf file. \n",
    "\n",
    "VCF_FILE_NAME = 'Tumor_RNAseq_variants.vcf'\n",
    "CSV_FILE_NAME = 'Tumor_RNAseq_variants.hg19_multianno.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"myvariant\"></a>\n",
    "## Getting data from myvariant.info\n",
    "The package offers 4 different methods to obtain variant data. Two of them require annovar, while the other two are based solely on the use of myvariant.info service. The latter can be used without having to worry about downloading and installing annovar databases, but it tends to return partial or no information for some variants. \n",
    "\n",
    "The different methods also enable the user to decide how the data will be parsed to MongoDB. 1 and 3 parse the data by chunks: the user specifies a number of variants (usually 1000), and the data from the vcf and csv files are parsed as soon as those 1000 variants are processed and integrated. This enables huge files to be processed without having to hold them in memory and potentially cause a Memory Overflow error. \n",
    "\n",
    "Methods 2 and 4, on the other hand, process the files on their entirety and send them to MongoDB at once. Well-suited for smaller files. See docs for more info. \n",
    "\n",
    "## Export data to MongoDB by chunks, iteratively. \n",
    "\n",
    "For this tutorial, we will use method #1. Data from annovar (as a csv file) will be obtained 1000 lines at a time, instead of attempting to parse and process an entire csv file at once.\n",
    "\n",
    "As soon as you run the scripts from variantannotaiton, variant data will be retrieved from myvariant.info and the data will automatically be integrated and stored to MongoDB. Database and collection name should be specified, and there must be a running MongoDB connection. The script will set up a client to communicate between python (through pymongo) and the the database.\n",
    "\n",
    "In general, the shell command:\n",
    "\n",
    "`mongod --dbpath ../data/db`  \n",
    "\n",
    "where data/db is the designated location where the data will be stored, will initiate MongoDB. After this, the script should store data to the directory automatically.\n",
    "For pymongo, and more information on how to set up a Mongo Database: https://docs.mongodb.com/getting-started/python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 1 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 2 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 3 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 4 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 5 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 6 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 7 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 8 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 9 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 10 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 11 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 12 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 13 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 14 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 15 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 16 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 17 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 18 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 19 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 20 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 21 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 22 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 23 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 24 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 25 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 26 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 27 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 28 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 29 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 30 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 31 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 32 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 33 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 34 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 35 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 36 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 37 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 38 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 39 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 40 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 41 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 42 of 43\n",
      "Converting columns to float ...\n",
      "Processing knownGene info ...\n",
      "Processing tfbsConsSites info ...\n",
      "Processing genomicSuperDups info ...\n",
      "Processing cytoBand info ...\n",
      "Creating hgvs key ...\n",
      "Processing genotype call info ...\n",
      "Transforming to JSON from dataFrame\n",
      "cleaning up...\n",
      "Done\n",
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-2710...done.\n",
      "Joining lists ...\n",
      "Parsing to MongoDB ...\n",
      "Step: 43 of 43\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished!'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunksize = 10000\n",
    "step = 0\n",
    "\n",
    "#Get variant list. Should always be the first step after running ANNOVAR\n",
    "open_file = myvariant_parsing_utils.VariantParsing()\n",
    "\n",
    "#Name Collections & DB. Change them to something appropriate. Each file should live in a collection\n",
    "db_name = 'Variant_Prioritization_Workflow'\n",
    "\n",
    "collection_name = 'Test_Tumor_RNAseq'\n",
    "\n",
    "list_file = open_file.get_variants_from_vcf(filepath_in+VCF_FILE_NAME)\n",
    "as_batch = annotate_batch.AnnotationMethods()\n",
    "as_batch.by_chunks(list_file, chunksize, step, filepath_out+CSV_FILE_NAME, collection_name, db_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"filter\"></a>\n",
    "## Variant Filtering & Output Files\n",
    "\n",
    "Here we implement three different filters that allow for the retrieval of specific variants. The filters are implemented as MongoDB queries, and are designed to provie the user with a set of relevant variants. In case the user would like to define its own querying, a template is provided. \n",
    "The output of the queries is a list of dictionaries (JSON documents), where each dictionary contains data reltive to one variant. \n",
    "\n",
    "Further, the package allows the user to parse these variants into an annotated csv or vcf file. \n",
    "If needed, annotated, unfiltered vcf and csv files can also be created. They will have the same length (number of variants) as the original files, but will contain much more complete annotation data coming from myvariant.info and ANNOVAR databases. \n",
    "\n",
    "To create a csv file, just the filtered output is needed. To create an annotated vcf file, a tab indexed file (.tbi) file is needed (see comments in  section Create unfiltered annotated vcf and csv files at the end of this page). This can be created using tabix.  \n",
    "\n",
    "First, the file needs to be compressed:\n",
    "\n",
    "From the command line, running:\n",
    "\n",
    "`bgzip -c input_file.vcf > input_file.vcf.gz`\n",
    "\n",
    "returns `input_vcf_file.vcf.gz`\n",
    "\n",
    "and running \n",
    "\n",
    "`tabix input_vcf_file.vcf.gz`\n",
    "\n",
    "will return: `input_vcf_file.vcf.gz.tbi`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"tumorvars\"></a>\n",
    "## Specifying cancer-specifc rare variants\n",
    "\n",
    " - filter 1: ThousandGenomeAll < 0.05 or info not available\n",
    " - filter 2: ESP6500siv2_all < 0.05 or info not available\n",
    " - filter 3: cosmic70 information is present\n",
    " - filter 4: Func_knownGene is exonic, splicing, or both\n",
    " - filter 5: ExonicFunc_knownGene is not \"synonymous SNV\"\n",
    " - filter 6: Read Depth (DP) > 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filepath = '/data/ccbb_internal/interns/Carlo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variants found that match rarity criteria: 11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished writing annotated, filtered VCF file'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create output files (if needed): specify name of files and path \n",
    "rare_cancer_variants_csv = filepath + \"/tumor_rna_rare_cancer_vars_csv.csv\"\n",
    "rare_cancer_variants_vcf = filepath + \"/tumor_rna_rare_cancer_vars_vcf.vcf\"\n",
    "input_vcf_compressed = filepath + '/test_vcf/Tumor_RNAseq_variants.vcf.gz'\n",
    "\n",
    "#Apply filter.\n",
    "filter_collection = MongoDB_querying.Filters(db_name, collection_name)\n",
    "rare_cancer_variants = filter_collection.rare_cancer_variant()\n",
    "\n",
    "#Crete writer object for filtered lists:\n",
    "my_writer = create_output_files.FileWriter(db_name, collection_name)\n",
    "\n",
    "#cancer variants filtered files\n",
    "my_writer.generate_annotated_csv(rare_cancer_variants, rare_cancer_variants_csv)\n",
    "my_writer.generate_annotated_vcf(rare_cancer_variants,input_vcf_compressed, rare_cancer_variants_vcf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"VarP\"></a>\n",
    "# Variant Prioritization\n",
    "## Read filtered vcf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from VarP import utils\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/carlomazzaferro/Documents/CCBB/neoantigen/VarP-master/')\n",
    "reader_names = ['Tumor_RNA_Reader', 'Tumor_Targeted_Reader',\n",
    "           'Normal_DNA_Reader', 'Normal_Blood_Reader',\n",
    "           'Somatic_Mutect_Reader']   #bug fixed in source\n",
    "\n",
    "path_to_files ='/Volumes/Seagate Backup Plus Drive 1/vcf_files/varcode_to_test/' \n",
    "myhandler = utils.HandleReaders(reader_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "### Create a list of variant collections. See varcode's documentation for more info on this type of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'varcode.variant_collection.VariantCollection'>\n"
     ]
    }
   ],
   "source": [
    "list_collections = myhandler.create_collection_from_readers(path_to_files)\n",
    "\n",
    "print type(list_collections[4])\n",
    "#Let's pick one of the collections to start working with\n",
    "my_collection = list_collections[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"out_fasta_df\"></a>\n",
    "### Create desired outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_coding_effects = myhandler.return_list_coding_effects(my_collection)   #list of coding effects\n",
    "protein_list = myhandler.return_protein_list(list_coding_effects)           #list of proteins\n",
    "dataframe = myhandler.return_dataframe(protein_list, list_coding_effects)   #dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prot</th>\n",
       "      <th>variants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MVSKLSQLQTELLAALLESGLSKEALIQALGEPGPYLLAGEGPLDK...</td>\n",
       "      <td>FrameShift(variant=chr12 g.121434630_121434631...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MVKSYLQQHNIPQREVVDTTGLNQSHLSQHLNKGTPMKTQKRAALY...</td>\n",
       "      <td>FrameShift(variant=chr12 g.121434630_121434631...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MEPSRALLGCLASAAAAAPPGEDGAGAGAEEEEEEEEAAAAVGPGE...</td>\n",
       "      <td>Deletion(variant=chr14 g.71275774_71275776delC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MEPSRALLGCLASAAAAAPPGEDGAGAGAEEEEEEEEAAAAVGPGE...</td>\n",
       "      <td>Deletion(variant=chr14 g.71275774_71275776delC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MEPSRALLGCLASAAAAAPPGEDGAGAGAEEEEEEEEAAAAVGPGE...</td>\n",
       "      <td>Deletion(variant=chr14 g.71275774_71275776delC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MSRRKQGNPQHLSQRELITPEADHVEAAILEEDEGLEIEEPSGLGL...</td>\n",
       "      <td>Deletion(variant=chr14 g.99641544_99641546delC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MSRRKQGNPQHLSQRELITPEADHVEAAILEEDEGLEIEEPSGLGL...</td>\n",
       "      <td>Deletion(variant=chr14 g.99641544_99641546delC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MSRRKQGNPQHLSQRELITRKDEPSSYICTTCKQPFNSAWFLLQHA...</td>\n",
       "      <td>Deletion(variant=chr14 g.99641544_99641546delC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MEGCDSPVVSGKDNGCGIPQHQQWTELNSTHLPDKPSSMEQSTGES...</td>\n",
       "      <td>Deletion(variant=chr16 g.72831358_72831360delT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MRLGGGQLVSEELMNLGESFIQTNDPSLKLFQCAVCNKFTTDNLDM...</td>\n",
       "      <td>Deletion(variant=chr16 g.72831358_72831360delT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MEGCDSPVVSGKDNGCGIPQHQQWTELNSTHLPDKPSSMEQSTGES...</td>\n",
       "      <td>Deletion(variant=chr16 g.72991713_72991715delC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MAPPSEETPLIPQRSCSLLSTEAGALHVLLPARGPGPPQRLSFSFG...</td>\n",
       "      <td>Substitution(variant=chr19 g.17948823C&gt;T, tran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MAPPSEETPLIPQRSCSLLSTEAGALHVLLPARGPGPPQRLSFSFG...</td>\n",
       "      <td>Substitution(variant=chr19 g.17948823C&gt;T, tran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MEARSRSAEELRRAELVEIIVETEAQTGVSGINVAGGGKEGIFVRE...</td>\n",
       "      <td>Deletion(variant=chr19 g.40900180_40900182delT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MSKKISGGSVVEMQGDEMTRIIWELIKEKLIFPYVELDLHSYDLGI...</td>\n",
       "      <td>Substitution(variant=chr2 g.209108317C&gt;T, tran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MKSNQERSNECLPPKKREIPATSRSSEEKAPTLPSDNHRVEGTAWL...</td>\n",
       "      <td>Insertion(variant=chr6 g.16327915_16327916insT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>MSTPTDPGAMPHPGPSPGPGPSPGPILGPSPGPGPSPGSVHSMMGP...</td>\n",
       "      <td>Deletion(variant=chr9 g.2039777_2039779delCAG,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>MSTPTDPGAMPHPGPSPGPGPSPGPILGPSPGPGPSPGSVHSMMGP...</td>\n",
       "      <td>Deletion(variant=chr9 g.2039777_2039779delCAG,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 prot  \\\n",
       "0   MVSKLSQLQTELLAALLESGLSKEALIQALGEPGPYLLAGEGPLDK...   \n",
       "1   MVKSYLQQHNIPQREVVDTTGLNQSHLSQHLNKGTPMKTQKRAALY...   \n",
       "2   MEPSRALLGCLASAAAAAPPGEDGAGAGAEEEEEEEEAAAAVGPGE...   \n",
       "3   MEPSRALLGCLASAAAAAPPGEDGAGAGAEEEEEEEEAAAAVGPGE...   \n",
       "4   MEPSRALLGCLASAAAAAPPGEDGAGAGAEEEEEEEEAAAAVGPGE...   \n",
       "5   MSRRKQGNPQHLSQRELITPEADHVEAAILEEDEGLEIEEPSGLGL...   \n",
       "6   MSRRKQGNPQHLSQRELITPEADHVEAAILEEDEGLEIEEPSGLGL...   \n",
       "7   MSRRKQGNPQHLSQRELITRKDEPSSYICTTCKQPFNSAWFLLQHA...   \n",
       "8   MEGCDSPVVSGKDNGCGIPQHQQWTELNSTHLPDKPSSMEQSTGES...   \n",
       "9   MRLGGGQLVSEELMNLGESFIQTNDPSLKLFQCAVCNKFTTDNLDM...   \n",
       "10  MEGCDSPVVSGKDNGCGIPQHQQWTELNSTHLPDKPSSMEQSTGES...   \n",
       "11  MAPPSEETPLIPQRSCSLLSTEAGALHVLLPARGPGPPQRLSFSFG...   \n",
       "13  MAPPSEETPLIPQRSCSLLSTEAGALHVLLPARGPGPPQRLSFSFG...   \n",
       "14  MEARSRSAEELRRAELVEIIVETEAQTGVSGINVAGGGKEGIFVRE...   \n",
       "15  MSKKISGGSVVEMQGDEMTRIIWELIKEKLIFPYVELDLHSYDLGI...   \n",
       "18  MKSNQERSNECLPPKKREIPATSRSSEEKAPTLPSDNHRVEGTAWL...   \n",
       "20  MSTPTDPGAMPHPGPSPGPGPSPGPILGPSPGPGPSPGSVHSMMGP...   \n",
       "21  MSTPTDPGAMPHPGPSPGPGPSPGPILGPSPGPGPSPGSVHSMMGP...   \n",
       "\n",
       "                                             variants  \n",
       "0   FrameShift(variant=chr12 g.121434630_121434631...  \n",
       "1   FrameShift(variant=chr12 g.121434630_121434631...  \n",
       "2   Deletion(variant=chr14 g.71275774_71275776delC...  \n",
       "3   Deletion(variant=chr14 g.71275774_71275776delC...  \n",
       "4   Deletion(variant=chr14 g.71275774_71275776delC...  \n",
       "5   Deletion(variant=chr14 g.99641544_99641546delC...  \n",
       "6   Deletion(variant=chr14 g.99641544_99641546delC...  \n",
       "7   Deletion(variant=chr14 g.99641544_99641546delC...  \n",
       "8   Deletion(variant=chr16 g.72831358_72831360delT...  \n",
       "9   Deletion(variant=chr16 g.72831358_72831360delT...  \n",
       "10  Deletion(variant=chr16 g.72991713_72991715delC...  \n",
       "11  Substitution(variant=chr19 g.17948823C>T, tran...  \n",
       "13  Substitution(variant=chr19 g.17948823C>T, tran...  \n",
       "14  Deletion(variant=chr19 g.40900180_40900182delT...  \n",
       "15  Substitution(variant=chr2 g.209108317C>T, tran...  \n",
       "18  Insertion(variant=chr6 g.16327915_16327916insT...  \n",
       "20  Deletion(variant=chr9 g.2039777_2039779delCAG,...  \n",
       "21  Deletion(variant=chr9 g.2039777_2039779delCAG,...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display dataframe\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Fasta file\n",
    "myhandler.generate_fasta_file(dataframe, path_to_files+'PEPTIDES.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"ep_pred\"></a>\n",
    "# New Epitope Prediction\n",
    "## Use epitopepredict and netMHCpan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For uveal melanoma, FrmaeShift mutations are quite rare and may be rejected based on literature studies:\n",
    "\n",
    "Instability of microsatellites is an infrequent event in uveal melanoma. **Cross NA, Murray AK, Rennie IG, Ganesh A, Sisley K**. Melanoma Res. 2003 Oct;13(5):435-40. PMID: 14512784\n",
    "\n",
    "\"Uveal melanomas do not frequently exhibit MSI, either at high levels or low levels. These observations provide further evidence for the theory that uveal melanomas and cutaneous melanomas represent distinctly different pathologies.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions done for 18 proteins in 4 alleles\n",
      "results saved to /Volumes/Seagate Backup Plus Drive 1/vcf_files/varcode_to_test/netmhciipan\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import epitopepredict as ep\n",
    "from epitopepredict import base, sequtils, analysis\n",
    "\n",
    "sns.set_context(\"notebook\", font_scale=1.4)\n",
    "fastafile =  path_to_files+'PEPTIDES.txt'\n",
    "\n",
    "#get data in DF format\n",
    "df = sequtils.fasta_to_dataframe(fastafile)\n",
    "\n",
    "P = base.get_predictor('netmhciipan')\n",
    "#P1 = base.getPredictor('netmhciipan')\n",
    "#P2 = base.getPredictor('netmhciipan')\n",
    "\n",
    "savepath1 = 'netmhciipan'\n",
    "#run prediction for several alleles and save results to savepath\n",
    "alleles = [\"HLA-DRB1*0101\",\"HLA-DRB1*0108\", \"HLA-DRB1*0305\", \"HLA-DRB1*0401\"]\n",
    "           #\"HLA-DRB1*0404\", \"HLA-DRB3*0101\", \"HLA-DRB4*0104\"]\n",
    "\n",
    "P.predictProteins(df, length=11, alleles=alleles, path=savepath1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary: 98444 peptides in 18 proteins and 4 alleles\n",
      "                                                    peptide\n",
      "name                                                       \n",
      "Deletion(variant=chr14-g.71275774_71275776delCC...     4360\n",
      "Deletion(variant=chr14-g.71275774_71275776delCC...     4416\n",
      "Deletion(variant=chr14-g.71275774_71275776delCC...     4268\n",
      "Deletion(variant=chr14-g.99641544_99641546delCT...     3520\n",
      "Deletion(variant=chr14-g.99641544_99641546delCT...     3236\n",
      "Deletion(variant=chr14-g.99641544_99641546delCT...     2744\n",
      "Deletion(variant=chr16-g.72831358_72831360delTT...    14756\n",
      "Deletion(variant=chr16-g.72831358_72831360delTT...    11100\n",
      "Deletion(variant=chr16-g.72991713_72991715delCC...    14756\n",
      "Deletion(variant=chr19-g.40900180_40900182delTC...     5788\n",
      "Deletion(variant=chr9-g.2039777_2039779delCAG-t...     6304\n",
      "Deletion(variant=chr9-g.2039777_2039779delCAG-t...     6232\n",
      "FrameShift(variant=chr12-g.121434630_121434631i...     1924\n",
      "FrameShift(variant=chr12-g.121434630_121434631i...     1456\n",
      "Insertion(variant=chr6-g.16327915_16327916insTG...     3212\n",
      "Substitution(variant=chr19-g.17948823C>T-transc...     4324\n",
      "Substitution(variant=chr19-g.17948823C>T-transc...     4444\n",
      "Substitution(variant=chr2-g.209108317C>T-transc...     1604\n"
     ]
    }
   ],
   "source": [
    "P.load(path = savepath1)\n",
    "P.summarize()\n",
    "P.allele_summary()\n",
    "P.protein_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_variant_name(x):\n",
    "    split_ = re.split(r\"[=-]+\",x)\n",
    "    var_name = split_[1]+':'+split_[2]\n",
    "    return var_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"clustering\"></a>\n",
    "## Clustering Analysis\n",
    "### Some definitions\n",
    "**Promiscuous Binder**: An antigen or region of antigen that can bind to several HLA alleles. These regions are most suitable for vaccine development because with single epitope, the immune response can be generated in large population.\n",
    "\n",
    "Would be interesting to implement CBS's [clustring toolkit](http://www.cbs.dtu.dk/services/MHCcluster-2.0/treeguide.php).\n",
    "Not available for download unfortunately, but it could be reconstructed from their [paper](http://link.springer.com/article/10.1007%2Fs00251-013-0714-9): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 clusters found in 15 proteins\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>binders</th>\n",
       "      <th>clustersize</th>\n",
       "      <th>peptide</th>\n",
       "      <th>variant_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Deletion(variant=chr14-g.99641544_99641546delC...</td>\n",
       "      <td>170</td>\n",
       "      <td>217</td>\n",
       "      <td>7</td>\n",
       "      <td>47</td>\n",
       "      <td>RRLRELAGNSSTPPPVSPGRGNPMHRLLNPFQPSPKSPFLSTPPLPP</td>\n",
       "      <td>chr14:g.99641544_99641546delCTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Insertion(variant=chr6-g.16327915_16327916insT...</td>\n",
       "      <td>118</td>\n",
       "      <td>147</td>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "      <td>QYAHLPHTFQFIGSSQYSGTYASFIPSQL</td>\n",
       "      <td>chr6:g.16327915_16327916insTGC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deletion(variant=chr16-g.72831358_72831360delT...</td>\n",
       "      <td>2219</td>\n",
       "      <td>2252</td>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>PGFTPSNTALTSPKPNLMGLPSTTVPSPGLPTS</td>\n",
       "      <td>chr16:g.72831358_72831360delTTG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Insertion(variant=chr6-g.16327915_16327916insT...</td>\n",
       "      <td>490</td>\n",
       "      <td>525</td>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>LLIPVGSTDMEASGAAPAIVTSSPQFAAVPHTFVT</td>\n",
       "      <td>chr6:g.16327915_16327916insTGC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Substitution(variant=chr19-g.17948823C&gt;T-trans...</td>\n",
       "      <td>590</td>\n",
       "      <td>619</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>CMAGDSTMVQEFVHLGAIDMYLRKRGHLV</td>\n",
       "      <td>chr19:g.17948823C&gt;T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Deletion(variant=chr16-g.72831358_72831360delT...</td>\n",
       "      <td>2028</td>\n",
       "      <td>2058</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>PGQKRFRTQMTNLQLKVLKSCFNDYRTPTM</td>\n",
       "      <td>chr16:g.72831358_72831360delTTG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Deletion(variant=chr16-g.72831358_72831360delT...</td>\n",
       "      <td>505</td>\n",
       "      <td>536</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>SQYHVIRAATMCCLCQRSFRTFQALKKHLET</td>\n",
       "      <td>chr16:g.72831358_72831360delTTG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FrameShift(variant=chr12-g.121434630_121434631...</td>\n",
       "      <td>228</td>\n",
       "      <td>260</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>TPLHQVSPTGLEPSHSLLSTEAKLVSAAGGPL</td>\n",
       "      <td>chr12:g.121434630_121434631insTCATTCAT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Deletion(variant=chr14-g.71275774_71275776delC...</td>\n",
       "      <td>691</td>\n",
       "      <td>724</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>GIHEEPTPVNSATSTPQLTPTNSLKRGGAHHRR</td>\n",
       "      <td>chr14:g.71275774_71275776delCCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Substitution(variant=chr2-g.209108317C&gt;T-trans...</td>\n",
       "      <td>270</td>\n",
       "      <td>297</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>NYDGDVQSDSVAQGYGSLGMMTSVLVC</td>\n",
       "      <td>chr2:g.209108317C&gt;T</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  start   end  binders  \\\n",
       "0  Deletion(variant=chr14-g.99641544_99641546delC...    170   217        7   \n",
       "1  Insertion(variant=chr6-g.16327915_16327916insT...    118   147        5   \n",
       "2  Deletion(variant=chr16-g.72831358_72831360delT...   2219  2252        5   \n",
       "3  Insertion(variant=chr6-g.16327915_16327916insT...    490   525        4   \n",
       "4  Substitution(variant=chr19-g.17948823C>T-trans...    590   619        4   \n",
       "5  Deletion(variant=chr16-g.72831358_72831360delT...   2028  2058        4   \n",
       "6  Deletion(variant=chr16-g.72831358_72831360delT...    505   536        4   \n",
       "7  FrameShift(variant=chr12-g.121434630_121434631...    228   260        4   \n",
       "8  Deletion(variant=chr14-g.71275774_71275776delC...    691   724        4   \n",
       "9  Substitution(variant=chr2-g.209108317C>T-trans...    270   297        4   \n",
       "\n",
       "   clustersize                                          peptide  \\\n",
       "0           47  RRLRELAGNSSTPPPVSPGRGNPMHRLLNPFQPSPKSPFLSTPPLPP   \n",
       "1           29                    QYAHLPHTFQFIGSSQYSGTYASFIPSQL   \n",
       "2           33                PGFTPSNTALTSPKPNLMGLPSTTVPSPGLPTS   \n",
       "3           35              LLIPVGSTDMEASGAAPAIVTSSPQFAAVPHTFVT   \n",
       "4           29                    CMAGDSTMVQEFVHLGAIDMYLRKRGHLV   \n",
       "5           30                   PGQKRFRTQMTNLQLKVLKSCFNDYRTPTM   \n",
       "6           31                  SQYHVIRAATMCCLCQRSFRTFQALKKHLET   \n",
       "7           32                 TPLHQVSPTGLEPSHSLLSTEAKLVSAAGGPL   \n",
       "8           33                GIHEEPTPVNSATSTPQLTPTNSLKRGGAHHRR   \n",
       "9           27                      NYDGDVQSDSVAQGYGSLGMMTSVLVC   \n",
       "\n",
       "                             variant_name  \n",
       "0         chr14:g.99641544_99641546delCTC  \n",
       "1          chr6:g.16327915_16327916insTGC  \n",
       "2         chr16:g.72831358_72831360delTTG  \n",
       "3          chr6:g.16327915_16327916insTGC  \n",
       "4                     chr19:g.17948823C>T  \n",
       "5         chr16:g.72831358_72831360delTTG  \n",
       "6         chr16:g.72831358_72831360delTTG  \n",
       "7  chr12:g.121434630_121434631insTCATTCAT  \n",
       "8         chr14:g.71275774_71275776delCCT  \n",
       "9                     chr2:g.209108317C>T  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get promiscuous binders in at least 2 alleles above 5 percentile cutoff\n",
    "name='chr16:g.72831358_72831360delTTG'\n",
    "pb = P.promiscuousBinders(n=2,cutoff=5)\n",
    "\n",
    "#find clusters of binders in these results\n",
    "cl = analysis.find_clusters(pb, dist=10, min_size=3, genome=zaire)\n",
    "cl[\"variant_name\"] = cl.name.apply(extract_variant_name)\n",
    "cl = cl.rename(columns={'name': 'gen_description', 'variant_name': 'name'})\n",
    "print cl.name.unique()\n",
    "display(cl[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Deletion(variant=chr14-g.71275774_71275776delCCT-transcript_name=MAP3K9-001-transcript_id=ENST00000554752-effect_description=p.E37del)'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.name.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions done for 18 proteins in 7 alleles\n",
      "results saved to /Volumes/Seagate Backup Plus Drive 1/vcf_files/varcode_to_test/test_netmhciipan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlomazzaferro/anaconda/lib/python2.7/site-packages/epitopepredict/analysis.py:167: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[label] = df.apply(lambda r: overlap(r,found),axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 with overlapping binders\n"
     ]
    }
   ],
   "source": [
    "Pn = ep.get_predictor('netmhciipan')\n",
    "savepath2='test_netmhciipan'\n",
    "alleles2 = ep.mhc2_supertypes\n",
    "Pn.predictProteins(zaire,length=11,alleles=alleles2,path=savepath2,overwrite=False)\n",
    "Pn.load(path=savepath2)\n",
    "\n",
    "cl = analysis.get_overlaps(cl,Pn.promiscuousBinders(n=2,cutoff=5),label='mhc2_ovlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 name  start  end  binders  \\\n",
      "23  Deletion(variant=chr14-g.71275774_71275776delC...    337  356        3   \n",
      "26  Deletion(variant=chr14-g.71275774_71275776delC...    556  572        3   \n",
      "39  Deletion(variant=chr14-g.71275774_71275776delC...    130  145        2   \n",
      "88  Deletion(variant=chr14-g.71275774_71275776delC...    215  230        2   \n",
      "\n",
      "    clustersize              peptide                     variant_name  \\\n",
      "23           19  LWELLTGEVPFRGIDGLAV  chr14:g.71275774_71275776delCCT   \n",
      "26           16     RLRAIQLTPGESSKTW  chr14:g.71275774_71275776delCCT   \n",
      "39           15      PPIQLLEIDFAELTL  chr14:g.71275774_71275776delCCT   \n",
      "88           15      CLVMEFARGGPLNRV  chr14:g.71275774_71275776delCCT   \n",
      "\n",
      "    mhc2_ovlp  \n",
      "23          2  \n",
      "26          3  \n",
      "39          2  \n",
      "88          2  \n"
     ]
    }
   ],
   "source": [
    "#plot both sets of binders and overlay region of cluster in previous data\n",
    "name = 'Deletion(variant=chr14-g.71275774_71275776delCCT-transcript_name=MAP3K9-001-transcript_id=ENST00000554752-effect_description=p.E37del)'\n",
    "ax = ep.plot_tracks([P,Pn],name=name,legend=True,figsize=(14,4),n=2)\n",
    "r = cl[cl.name==name]\n",
    "print r\n",
    "coords = (list(r.start),list(r.end-r.start))\n",
    "coords = zip(*coords)\n",
    "ep.plot_regions(coords, ax, color='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"conservation\"></a>\n",
    "## Conservation Analysis\n",
    "Conservation of epitopes may be important across strains or species.\n",
    "\n",
    "\n",
    "###### Need to fix errors and disp plots. May need to wait for update on repo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 non-redundant sequences retrieved\n"
     ]
    },
    {
     "ename": "ApplicationError",
     "evalue": "Non-zero return code 127 from 'muscle -in ebola_VP35.fa -out ebola_VP35.txt', message '/bin/sh: muscle: command not found'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mApplicationError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-26174487df4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_protein_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Filovirus[Orgn] AND VP35[Gene]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#align fasta sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0maln\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmuscle_alignment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m#sequtils.showAlignment(aln)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0malnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malignment_to_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maln\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/carlomazzaferro/anaconda/lib/python2.7/site-packages/epitopepredict/sequtils.pyc\u001b[0m in \u001b[0;36mmuscle_alignment\u001b[0;34m(filename, seqs)\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mBio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAlign\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mApplications\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMuscleCommandline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0mcline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMuscleCommandline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m     \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m     \u001b[0malign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAlignIO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fasta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/carlomazzaferro/anaconda/lib/python2.7/site-packages/Bio/Application/__init__.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, stdin, stdout, stderr, cwd, env)\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m             raise ApplicationError(return_code, str(self),\n\u001b[0;32m--> 516\u001b[0;31m                                    stdout_str, stderr_str)\n\u001b[0m\u001b[1;32m    517\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstdout_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mApplicationError\u001b[0m: Non-zero return code 127 from 'muscle -in ebola_VP35.fa -out ebola_VP35.txt', message '/bin/sh: muscle: command not found'"
     ]
    }
   ],
   "source": [
    "reload(ep.base)\n",
    "reload(analysis)\n",
    "reload(sequtils)\n",
    "\n",
    "name = 'Deletion(variant=chr14-g.71275774_71275776delCCT-transcript_name=MAP3K9-001-transcript_id=ENST00000554752-effect_description=p.E37del)'\n",
    "proteinseq = zaire[zaire.locus_tag==name].translation.iloc[0]\n",
    "#print proteinseq\n",
    "#print pb\n",
    "seqs = pb.peptide\n",
    "\n",
    "#provide a list of seqs (e.g. strains)\n",
    "filename='ebola_VP35.fa'\n",
    "r = sequtils.fetch_protein_sequences('Filovirus[Orgn] AND VP35[Gene]', filename=filename)\n",
    "#align fasta sequences\n",
    "aln = sequtils.muscle_alignment(filename)\n",
    "#sequtils.showAlignment(aln)\n",
    "alnrows = sequtils.alignment_to_dataframe(aln)\n",
    "\n",
    "#print (sequtils.formatAlignment(aln))\n",
    "#print alnrows[:25][['accession','definition','perc_ident']]\n",
    "c = analysis.epitope_conservation(seqs, alnrows=alnrows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
